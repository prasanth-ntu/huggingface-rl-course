{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e1a19b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#From-Q-Learning-to-Deep-Q-Learning\" data-toc-modified-id=\"From-Q-Learning-to-Deep-Q-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>From Q-Learning to Deep Q-Learning</a></span></li><li><span><a href=\"#The-Deep-Q-Network-(DQN)\" data-toc-modified-id=\"The-Deep-Q-Network-(DQN)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The Deep Q-Network (DQN)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing-the-input-and-temporal-limitation\" data-toc-modified-id=\"Preprocessing-the-input-and-temporal-limitation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Preprocessing the input and temporal limitation</a></span></li></ul></li><li><span><a href=\"#The-Deep-Q-Algorithm\" data-toc-modified-id=\"The-Deep-Q-Algorithm-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>The Deep Q Algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#Experience-Replay-to-make-more-efficient-use-of-experiences\" data-toc-modified-id=\"Experience-Replay-to-make-more-efficient-use-of-experiences-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Experience Replay to make more efficient use of experiences</a></span></li><li><span><a href=\"#Fixed-Q-Target-to-stabilize-the-training\" data-toc-modified-id=\"Fixed-Q-Target-to-stabilize-the-training-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Fixed Q-Target to stabilize the training</a></span></li><li><span><a href=\"#Double-DQN-to-handle-the-problem-of-overestimation-of-the-Q-values\" data-toc-modified-id=\"Double-DQN-to-handle-the-problem-of-overestimation-of-the-Q-values-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Double DQN to handle the problem of overestimation of the Q-values</a></span></li></ul></li><li><span><a href=\"#Glossary\" data-toc-modified-id=\"Glossary-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Glossary</a></span></li><li><span><a href=\"#Hands-on\" data-toc-modified-id=\"Hands-on-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Hands-on</a></span><ul class=\"toc-item\"><li><span><a href=\"#SpacesInvadersNoFrameskip-v4\" data-toc-modified-id=\"SpacesInvadersNoFrameskip-v4-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>SpacesInvadersNoFrameskip-v4</a></span></li></ul></li><li><span><a href=\"#Quiz\" data-toc-modified-id=\"Quiz-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Quiz</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Additional-Readings\" data-toc-modified-id=\"Additional-Readings-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Additional Readings</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704fe7a",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7b64b",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg\" style=\"width:600px;\" title=\"Unit 3 thumbnail\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4719ce",
   "metadata": {},
   "source": [
    "In the last unit, we learned our first reinforcement learning algorithm: Q-Learning, <b>implemented it from scratch</b>, and trained it in two environments, `FrozenLake-v1` ‚òÉÔ∏è and `Taxi-v3` üöï.\n",
    "\n",
    "We got excellent results with this simple algorithm, but these environments were relatively simple because the <b>state space was discrete and small </b>(16 different states for FrozenLake-v1 and 500 for Taxi-v3). For comparison, the state space in Atari games can <b>contain $10^{9}$ to $10^{11}$ states.</b>\n",
    "\n",
    "But as we‚Äôll see, <span style=\"color:red\">producing and updating a <b>Q-table can become ineffective in large state space environments.</b></span>\n",
    "\n",
    "So in this unit, <span style=\"color:blue\"><b>we‚Äôll study our first Deep Reinforcement Learning agent</b>: Deep Q-Learning. Instead of using a Q-table, Deep Q-Learning uses a Neural Network that takes a state and approximates Q-values for each action based on that state.</span>\n",
    "\n",
    "And <b>we‚Äôll train it to play Space Invaders and other Atari environments using [RL-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)</b>, a training framework for RL using Stable-Baselines that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results, and recording videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c4bbb",
   "metadata": {},
   "source": [
    "# From Q-Learning to Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf779297",
   "metadata": {},
   "source": [
    "We learned that <b>Q-Learning is an algorithm we use to train our Q-Function, an action-value function</b> that determines the value of being at a particular state and taking a specific action at that state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc4a03",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg\" style=\"width:700px;\" title=\"Q-function\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680bdfe1",
   "metadata": {},
   "source": [
    "The <b>Q comes from ‚Äúthe Quality‚Äù of that action at that state.</b>\n",
    "\n",
    "Internally, our Q-function is encoded by a <b>Q-table, a table where each cell corresponds to a state-action pair value</b>. Think of this Q-table as the <b>memory or cheat sheet of our Q-function.</b>\n",
    "\n",
    "<span style=\"color:red\">The problem is that Q-Learning is a <i>tabular method</i>. This becomes a problem if the states and actions spaces <b>are not small enough to be represented efficiently by arrays and tables</b>. In other words: it is <b>not scalable</b></span>. Q-Learning worked well with small state space environments like:\n",
    "\n",
    "- FrozenLake, we had 16 states.\n",
    "- Taxi-v3, we had 500 states.\n",
    "\n",
    "But think of what we‚Äôre going to do today: we will train an agent to learn to play Space Invaders, a more complex game, using the frames as input.\n",
    "\n",
    "As [Nikita Melkozerov mentioned](https://twitter.com/meln1k), <b>Atari environments</b> have an observation space with a shape of (210, 160, 3)*, containing values ranging from 0 to 255 so that gives us $256^{210 \\times160 \\times 3} =  256^{100800}$ possible observations (for comparison, we have approximately $10^{80}$ atoms in the observable universe).\n",
    "\n",
    "- A single frame in Atari is composed of an image of 210x160 pixels. Given that the images are in color (RGB), there are 3 channels. This is why the shape is (210, 160, 3). For each pixel, the value can go from 0 to 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99c627",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari.jpg\" style=\"width:600px;\" title=\"Atari State Space\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75523a",
   "metadata": {},
   "source": [
    "Therefore, <span style=\"color:blue\">the state space is gigantic;</span><span style=\"color:red\"> due to this, creating and updating a Q-table for that environment would not be efficient.</span><span style=\"color:green\"> In this case, the best idea is to approximate the Q-values using a parametrized Q-function $Q_{\\theta}(s,a)$.</span>\n",
    "\n",
    "This <b>neural network will approximate, given a state, the different Q-values for each possible action at that state. And that‚Äôs exactly what Deep Q-Learning does.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" style=\"width:600px;\" title=\"Deep Q Learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764fee2a",
   "metadata": {},
   "source": [
    "Now that we understand Deep Q-Learning, let‚Äôs dive deeper into the Deep Q-Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b5861",
   "metadata": {},
   "source": [
    "# The Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0076100",
   "metadata": {},
   "source": [
    "This is the architecture of our Deep Q-Learning network:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg\" style=\"width:600px;\" title=\"Deep Q Network\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc4326",
   "metadata": {},
   "source": [
    "As input, we take a <b>stack of 4 frames</b> passed through the network as a state and output a <b>vector of Q-values for each possible action at that state</b>. Then, like with Q-Learning, we just need to use our epsilon-greedy policy to select which action to take.\n",
    "\n",
    "When the Neural Network is initialized, <b>the Q-value estimation is terrible</b>. But during training, our Deep Q-Network agent will associate a situation with the appropriate action and <b>learn to play the game well.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a8874",
   "metadata": {},
   "source": [
    "## Preprocessing the input and temporal limitation\n",
    "\n",
    "We need to <b>preprocess the input</b>. It‚Äôs an <span style=\"color:blue\">essential step since we want to <b>reduce the complexity of our state to reduce the computation time needed for training.</b></span>\n",
    "\n",
    "To achieve this, we <b>reduce the state space to 84x84 and grayscale it</b>. We can do this since the colors in Atari environments don‚Äôt add important information. This is a big improvement since we <b>reduce our three color channels (RGB) to 1.</b> \n",
    "\n",
    "> <span style=\"color:blue\"><b>16x reduction</b> from $\\Rightarrow 210*160*3=113,400$ pixels in a frame to $84*84*1=7,056$ pixels.</span>\n",
    "\n",
    "We can also <b>crop a part of the screen in some games</b> if it does not contain important information. Then we stack four frames together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d5919",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/preprocessing.jpg\" style=\"width:600px;\" title=\"Preprocessing\">\n",
    "\n",
    "<span style=\"color:blue\"><b>Why do we stack four frames together</b>? We stack frames together because it helps us <b>handle the problem of temporal limitation</b></span>. Let‚Äôs take an example with the game of Pong. When you see this frame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9ae8c",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation.jpg\" style=\"width:600px;\" title=\"Temporal Limitation\">\n",
    "\n",
    "Can you tell me where the ball is going? No, because one frame is not enough to have a sense of motion! But what if I add three more frames? <b>Here you can see that the ball is going to the right.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870cec50",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation-2.jpg\" style=\"width:600px;\" title=\"Temporal Limitation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347e929",
   "metadata": {},
   "source": [
    "That‚Äôs why, to capture temporal information, we stack four frames together.\n",
    "\n",
    "Then the stacked frames are processed by three convolutional layers. These layers <b>allow us to capture and exploit spatial relationships in images</b>. But also, because the frames are stacked together, <b>we can exploit some temporal properties across those frames.</b>\n",
    "\n",
    "If you don‚Äôt know what convolutional layers are, don‚Äôt worry. You can check out [Lesson 4 of this free Deep Learning Course by Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n",
    "\n",
    "Finally, we have a couple of fully connected layers that output a Q-value for each possible action at that state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6bfb1",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg\" style=\"width:600px;\" title=\"Deep Q Network\">\n",
    "\n",
    "So, we see that Deep Q-Learning uses a neural network to approximate, given a state, the different Q-values for each possible action at that state. Now let‚Äôs study the Deep Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5767c41",
   "metadata": {},
   "source": [
    "# The Deep Q Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b6f8f",
   "metadata": {},
   "source": [
    "We learned that Deep Q-Learning <b>uses a deep neural network to approximate the different Q-values for each possible action at a state </b>(value-function estimation).\n",
    "\n",
    "The difference is that, during the training phase, instead of updating the Q-value of a state-action pair directly as we have done with Q-Learning:\n",
    "\n",
    "$$Q^{new}(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha[R_{t+1} + \\gamma \\text{max}_{a}Q(S_{t+1}, a) - Q(S_{t}, A_{t})] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a86bf6",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg\" style=\"width:700px;\" title=\"Q Loss\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c82075",
   "metadata": {},
   "source": [
    "in Deep Q-Learning, we create a <b>loss function that compares our Q-value prediction and the Q-target and uses gradient descent to update the weights of our Deep Q-Network to approximate our Q-values better.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg\" style=\"width:700px;\" title=\"Q-target\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed990b",
   "metadata": {},
   "source": [
    "- <b>Q-Target</b> or <b>TD Target</b>: \n",
    "    - $y_{j} = r_{j} + \\gamma max_{a} \\hat{Q}(\\psi_{j+1}, a'; \\theta^{-})$\n",
    "    - $y_{j} =  R_{t+1} + \\gamma max_{a}Q(S_{t+1},a)$\n",
    "- <b>Q-value prediction</b>\n",
    "    - $Q(\\psi_{j}, a_{j}; \\theta)$\n",
    "- <b>Q-loss (a.k.a.) TD error</b>: Q-target - Q-prediction\n",
    "    - $y_{j} - Q(\\psi_{j}, a_{j}; \\theta)$\n",
    "    - $R_{t+1} + \\gamma max_{a}Q(S_{t+1},a) - Q(S_{t}, A_{t})$\n",
    "\n",
    "where <b>(I guess)</b><br>\n",
    "- <span style=\"font-size:large\">$\\phi$</span> is the state<br>\n",
    "    - <span style=\"font-size:large\">$\\phi_{j}$</span> is the state at step $j$ and $\\phi_{j+1}$ is the state at step $j+1$<br>\n",
    "- <span style=\"font-size:large\">$a_{j}$</span> is the action at step $j$<br>\n",
    "- <span style=\"font-size:large\">$r_{j}$</span> is the reward at step $j$<br>\n",
    "    - <span style=\"color:#FF6133\">What does $-$ in the $\\theta^{-}$ mean? I guess, it means this is the [Fixed Q-target](#fixed-q-target), and $\\theta$ without $-$ means it's the Q-value prediction.</span>\n",
    "    - Why is there a ^ in $\\hat{Q}$\n",
    "- <span style=\"font-size:large\">$y_{j}$</span> is the <span style=\"color:blue\">Q-Target</span><br>\n",
    "- <span style=\"font-size:large\">$Q(\\phi_{j}, a_{j}; \\theta)$</span> is the <span style=\"color:blue\">current Q-value (estimation of Q) </span>\n",
    "    - ??? by our Deep Q-Network by learning optimal weights $\\theta$ using gradient descent</span>\n",
    "- <span style=\"font-size:large\">$y_{j} - Q(\\phi_{j}, a_{j}; \\theta)$</span> is the Q-Loss a.k.a. TD Error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a166e",
   "metadata": {},
   "source": [
    "The Deep Q-Learning training algorithm has <i>two phases</i>:\n",
    "\n",
    "- <b>Sampling</b>: We perform actions and <b>store the observed experience tuples in a replay memory.</b>\n",
    "- <b>Training</b>: Select a <b>small batch of tuples randomly and learn from this batch using a gradient descent update step.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0dd2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T09:28:29.332588Z",
     "start_time": "2023-08-20T09:28:29.316478Z"
    }
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/sampling-training.jpg\" style=\"width:700px;\" title=\"Sampling Training\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677749f4",
   "metadata": {},
   "source": [
    "This is not the only difference compared with Q-Learning. <span style=\"color:red\">Deep Q-Learning training <b>might suffer from instability</b>, mainly because of combining a non-linear Q-value function (Neural Network) and bootstrapping (when we update targets with existing estimates and not an actual complete return).</span>\n",
    "\n",
    "To help us stabilize the training, we implement three different solutions:\n",
    "\n",
    "1. <i>Experience Replay</i> to make more <b>efficient use of experiences.</b>\n",
    "2. <i>Fixed Q-Target</i> <b>to stabilize the training.</b>\n",
    "3. <i>Double Deep Q-Learning</i>, to <b>handle the problem of the overestimation of Q-values.</b>\n",
    "\n",
    "Let‚Äôs go through them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15aa92",
   "metadata": {},
   "source": [
    "## Experience Replay to make more efficient use of experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6deaa4",
   "metadata": {},
   "source": [
    "Why do we create a replay memory?\n",
    "\n",
    "Experience Replay in Deep Q-Learning has two functions:\n",
    "\n",
    "1. <b>Make more efficient use of the experiences during the training</b>. <span style=\"color:red\">Usually, in online reinforcement learning, the agent interacts with the environment, gets experiences (state, action, reward, and next state), learns from them (updates the neural network), and discards them. This is not efficient.</span>\n",
    "\n",
    "<span style=\"color:green\">Experience replay helps by <b>using the experiences of the training more efficiently.</b> We use a replay buffer that saves experience samples <b>that we can reuse during the training.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193f86e",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay.jpg\" style=\"width:700px;\" title=\"Experience Replay\">\n",
    "\n",
    "‚áí This allows the agent to <b>learn from the same experiences multiple times.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f1265",
   "metadata": {},
   "source": [
    "2. <b>Avoid forgetting previous experiences and reduce the correlation between experiences.</b>\n",
    "\n",
    "- <span style=\"color:red\">The problem we get if we give sequential samples of experiences to our neural network is that it tends to forget the <b>previous experiences as it gets new experiences</b>. For instance, if the agent is in the first level and then in the second, which is different, it can forget how to behave and play in the first level.</span>\n",
    "\n",
    "<span style=\"color:green\">The solution is to create a Replay Buffer that stores experience tuples while interacting with the environment and then sample a small batch of tuples. This prevents <b>the network from only learning about what it has done immediately before.</b></span>\n",
    "\n",
    "<span style=\"color:green\">Experience replay also has other benefits. By randomly sampling the experiences, we remove correlation in the observation sequences and avoid <b>action values from oscillating or diverging catastrophically.</b></span>\n",
    "\n",
    "In the Deep Q-Learning pseudocode, we <b>initialize a replay memory buffer D with capacity N</b> (N is a hyperparameter that you can define). We then store experiences in the memory and sample a batch of experiences to feed the Deep Q-Network during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5762d",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay-pseudocode.jpg\" style=\"width:700px;\" title=\"Experience Replay Pseudocode\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d71de",
   "metadata": {},
   "source": [
    "<a id=\"fixed-q-target\"></a>\n",
    "## Fixed Q-Target to stabilize the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e85760",
   "metadata": {},
   "source": [
    "When we want to calculate the TD error (aka the loss), we calculate the <b>difference between the TD target (Q-Target) and the current Q-value (estimation of Q).</b>\n",
    "\n",
    "<span style=\"color:red\">But we <b>don‚Äôt have any idea of the real TD target.</b></span> We need to estimate it. <span style=\"color:green\">Using the Bellman equation, we saw that the TD target is just the reward of taking that action at that state plus the discounted highest Q value for the next state.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb5d6c",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg\" style=\"width:700px;\" title=\"Q-target\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80cd29",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">However, the problem is that we are using the same parameters (weights) for estimating the TD target ($y_{j}$) and the Q-value ($Q(\\phi_{j}, a_{j}; \\theta)$). Consequently, there is a significant correlation between the TD target and the parameters we are changing.</span>\n",
    "\n",
    "Therefore, at every step of training,<b> both our Q-values and the target values shift</b>. We‚Äôre getting closer to our target, but the target is also moving. It‚Äôs like chasing a moving target! This can lead to significant oscillation in training.\n",
    "\n",
    "It‚Äôs like if you were a cowboy (the Q estimation) and you wanted to catch a cow (the Q-target). Your goal is to get closer (reduce the error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb87b7d",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-1.jpg\" style=\"width:400px;\" title=\"Q-target\">\n",
    "\n",
    "At each time step, you‚Äôre trying to approach the cow, which also moves at each time step (because you use the same parameters).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-2.jpg\" style=\"width:400px;\" title=\"Q-target\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-3.jpg\" style=\"width:400px;\" title=\"-target\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aaa7d5",
   "metadata": {},
   "source": [
    "This leads to a bizarre path of chasing (a significant oscillation in training).\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-4.jpg\" style=\"width:400px;\" title=\"Q-target\">\n",
    "\n",
    "Instead, what we see in the pseudo-code is that we:\n",
    "\n",
    "- Use a <b>separate network with fixed parameters</b> for estimating the TD Target\n",
    "- <b>Copy the parameters from our Deep Q-Network every C steps</b> to update the target network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e955fc5",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/fixed-q-target-pseudocode.jpg\" style=\"width:700px;\" title=\"Fixed Q-target Pseudocode\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78a595",
   "metadata": {},
   "source": [
    "## Double DQN to handle the problem of overestimation of the Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4b9ca",
   "metadata": {},
   "source": [
    "Double DQNs, or Double Deep Q-Learning neural networks, were introduced by [Hado van Hasselt](https://papers.nips.cc/paper/3964-double-q-learning). This method <b>handles the problem of the overestimation of Q-values.</b>\n",
    "\n",
    "To understand this problem, remember how we calculate the TD Target:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c24754",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" style=\"width:600px;\" title=\"TD target\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0323fe",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">We face a simple problem by calculating the TD target: how are we sure that <b>the best action for the next state is the action with the highest Q-value?</b></span>\n",
    "\n",
    "We know that the accuracy of Q-values depends on what action we tried <b>and</b> what neighboring states we explored.\n",
    "\n",
    "<span style=\"color:red\">Consequently, we don‚Äôt have enough information about the best action to take at the beginning of the training. Therefore, taking the maximum Q-value (which is noisy) as the best action to take can lead to false positives. If non-optimal actions are regularly <b>given a higher Q value than the optimal best action, the learning will be complicated.</b></span>\n",
    "\n",
    "<span style=\"color:green\">The solution is: when we compute the Q target, we use two networks to decouple the action selection from the target Q-value generation.</span> We:\n",
    "- Use our <b>DQN network</b> to select the best action to take for the next state (the action with the highest Q-value).\n",
    "- Use our <b>Target network</b> to calculate the target Q-value of taking that action at the next state.\n",
    "Therefore, Double DQN helps us reduce the overestimation of Q-values and, as a consequence, helps us train faster and with more stable learning.\n",
    "\n",
    "Since these three improvements in Deep Q-Learning, many more have been added, such as Prioritized Experience Replay and Dueling Deep Q-Learning. They‚Äôre out of the scope of this course but if you‚Äôre interested, check the links we put in the reading list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616dd2f",
   "metadata": {},
   "source": [
    "# Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9677a429",
   "metadata": {},
   "source": [
    "- <b>Tabular Method</b>: Type of problem in which the state and action spaces are small enough to approximate value functions to be represented as arrays and tables. <b>Q-learning</b> is an example of tabular method since a table is used to represent the value for different state-action pairs.\n",
    "\n",
    "- <b>Deep Q-Learning</b>: Method that trains a neural network to approximate, given a state, the different <b>Q-values</b> for each possible action at that state. It is used to solve problems when observational space is too big to apply a tabular Q-Learning approach.\n",
    "\n",
    "- <b>Temporal Limitation</b> is a difficulty presented when the environment state is represented by frames. A frame by itself does not provide temporal information. In order to obtain temporal information, we need to <b>stack</b> a number of frames together.\n",
    "\n",
    "- <b>Phases of Deep Q-Learning:</b>\n",
    "    - <b>Sampling</b>: Actions are performed, and observed experience tuples are stored in a <b>replay memory.</b>\n",
    "    - <b>Training</b>: Batches of tuples are selected randomly and the neural network updates its weights using gradient descent.\n",
    "\n",
    "- <b>Solutions to stabilize Deep Q-Learning:</b>\n",
    "    - <b>Experience Replay</b>: A replay memory is created to save experiences samples that can be reused during training. This allows the agent to learn from the same experiences multiple times. Also, it helps the agent avoid forgetting previous experiences as it gets new ones.\n",
    "    - <b>Random sampling</b> from replay buffer allows to remove correlation in the observation sequences and prevents action values from oscillating or diverging catastrophically.\n",
    "    - <b>Fixed Q-Target</b>: In order to calculate the <b>Q-Target</b> we need to estimate the discounted optimal <b>Q-value</b> of the next state by using Bellman equation. The problem is that the same network weights are used to calculate the <b>Q-Target</b> and the <b>Q-value</b>. This means that everytime we are modifying the <b>Q-value</b>, the <b>Q-Target</b> also moves with it. To avoid this issue, a separate network with fixed parameters is used for estimating the Temporal Difference Target. The target network is updated by copying parameters from our Deep Q-Network after certain <b>C steps</b>.\n",
    "\n",
    "    - <b>Double DQN</b>: Method to handle <b>overestimation of Q-Values</b>. This solution uses two networks to decouple the action selection from the target <b>Value generation:</b>\n",
    "        - <b>DQN Network</b> to select the best action to take for the next state (the action with the highest <b>Q-Value)</b>\n",
    "        - <b>Target Network</b> to calculate the target <b>Q-Value</b> of taking that action at the next state. This approach reduces the Q-Values overestimation, it helps to train faster and have more stable learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e21ea",
   "metadata": {},
   "source": [
    "# Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfd182",
   "metadata": {},
   "source": [
    "## SpacesInvadersNoFrameskip-v4\n",
    "\n",
    "- My [model card](https://huggingface.co/prasanthntu/q-Taxi-v3) in huggingface.\n",
    "    - It can be found under the `Reinforcement learning` [model libraries](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=trending&search=prasanthntu) section.\n",
    "- My [source code](https://colab.research.google.com/drive/1SSmGmegUEqArlttm9nM1omkkBefxt-VU?usp=sharing) in Google Colab stored in Google Drive for building the model.\n",
    "\n",
    "<b>Summary</b>\n",
    "- <b>Goal</b>:  Train our agent to navigate to passengers in a grid world, picking them up and dropping them off at one of four designated locations.\n",
    "- <b>Environment</b>\n",
    "    - [Gymnasium](https://gymnasium.farama.org/environments/toy_text/taxi/) - To create RL environments  \n",
    "        - Taxi-v3 \n",
    "            - Observation space: 1D vector\n",
    "                - `Discrete(500)` as there are \n",
    "                    - (5x5 grid) 25 taxi positions\n",
    "                    - 5 passenger locations (technically, 4 passenger locations + case when passenger is in the taxi)\n",
    "                    - 4 destination locations\n",
    "            - Action space: Scalar\n",
    "                - `Discrete(6)` $\\Rightarrow$ No. of actions available: 6\n",
    "                - Possible actions\n",
    "                    - 0: Move south (down)\n",
    "                    - 1: Move north (up)\n",
    "                    - 2: Move east (right)\n",
    "                    - 3: Move west (left)\n",
    "                    - 4: Pickup passenger\n",
    "                    - 5: Drop off passenger\n",
    "            - Rewards\n",
    "                - -1 per step unless other reward is triggered.\n",
    "                - +20 delivering passenger.\n",
    "                - -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally.\n",
    "                \n",
    "              An action that results a noop, like moving into a wall, will incur the time step penalty. Noops can be avoided by sampling the action_mask returned in info.\n",
    "              \n",
    "            - Episode end\n",
    "                - Termination\n",
    "                    - The taxi drops off the passenger. üëç                 \n",
    "                - Truncation\n",
    "                    - When exceeding the length of the interactions in a episode üëé\n",
    "                        - 200 timesteps.\n",
    "- <b>Model</b>\n",
    "    - Q learning - Built from scratch \n",
    "        - Q table shape: n_states x n_actions: 500 x 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7fc875",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007822a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb0fd3ab",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1f37b",
   "metadata": {},
   "source": [
    "Congrats on finishing this chapter! There was a lot of information. And congrats on finishing the tutorial. You‚Äôve just trained your first Deep Q-Learning agent and shared it on the Hub ü•≥.\n",
    "\n",
    "Take time to really grasp the material before continuing.\n",
    "\n",
    "Don‚Äôt hesitate to train your agent in other environments (Pong, Seaquest, QBert, Ms Pac Man). The <b>best way to learn is to try things on your own!</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" style=\"width:600px;\" title=\"Environments\">\n",
    "\n",
    "In the next unit, <b>we‚Äôre going to learn about Optuna</b>. One of the most critical tasks in Deep Reinforcement Learning is to find a good set of training hyperparameters. Optuna is a library that helps you to automate the search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc45488",
   "metadata": {},
   "source": [
    "# Additional Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ededa5ab",
   "metadata": {},
   "source": [
    "These are <b>optional readings</b> if you want to go deeper.\n",
    "\n",
    "- [Foundations of Deep RL Series, L2 Deep Q-Learning by Pieter Abbeel](https://youtu.be/Psrhxy88zww)\n",
    "- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)\n",
    "- [Double Deep Q-Learning](https://papers.nips.cc/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html)\n",
    "- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd999454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
