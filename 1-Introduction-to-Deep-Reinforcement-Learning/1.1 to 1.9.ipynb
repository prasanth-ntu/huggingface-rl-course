{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3a255a",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#What-is-Reinforcement-Learning?\" data-toc-modified-id=\"What-is-Reinforcement-Learning?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What is Reinforcement Learning?</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-big-picture\" data-toc-modified-id=\"The-big-picture-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>The big picture</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-formal-definition\" data-toc-modified-id=\"The-formal-definition-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>The formal definition</a></span></li></ul></li></ul></li><li><span><a href=\"#The-Reinforcement-Learning-Framework\" data-toc-modified-id=\"The-Reinforcement-Learning-Framework-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The Reinforcement Learning Framework</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-RL-Process\" data-toc-modified-id=\"The-RL-Process-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>The RL Process</a></span></li><li><span><a href=\"#The-reward-hypothesis:-the-central-idea-of-reinforcement-learning\" data-toc-modified-id=\"The-reward-hypothesis:-the-central-idea-of-reinforcement-learning-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The reward hypothesis: the central idea of reinforcement learning</a></span></li><li><span><a href=\"#Markov-property\" data-toc-modified-id=\"Markov-property-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Markov property</a></span></li><li><span><a href=\"#Observations/States-Space\" data-toc-modified-id=\"Observations/States-Space-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Observations/States Space</a></span></li><li><span><a href=\"#Action-Space\" data-toc-modified-id=\"Action-Space-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Action Space</a></span></li><li><span><a href=\"#Rewards-and-the-discounting\" data-toc-modified-id=\"Rewards-and-the-discounting-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Rewards and the discounting</a></span></li></ul></li><li><span><a href=\"#The-type-of-tasks\" data-toc-modified-id=\"The-type-of-tasks-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>The type of tasks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Episodic-task\" data-toc-modified-id=\"Episodic-task-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Episodic task</a></span></li><li><span><a href=\"#Continuous-task\" data-toc-modified-id=\"Continuous-task-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Continuous task</a></span></li></ul></li><li><span><a href=\"#The-Exploration/-Exploitation-tradeoff\" data-toc-modified-id=\"The-Exploration/-Exploitation-tradeoff-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>The Exploration/ Exploitation tradeoff</a></span></li><li><span><a href=\"#The-two-main-approaches-for-solving-RL-problems\" data-toc-modified-id=\"The-two-main-approaches-for-solving-RL-problems-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>The two main approaches for solving RL problems</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-policy-π:-the-agent’s-brain\" data-toc-modified-id=\"The-policy-π:-the-agent’s-brain-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>The policy π: the agent’s brain</a></span></li><li><span><a href=\"#Policy-Based-methods\" data-toc-modified-id=\"Policy-Based-methods-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Policy-Based methods</a></span></li><li><span><a href=\"#Value-Based-methods\" data-toc-modified-id=\"Value-Based-methods-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Value-Based methods</a></span></li></ul></li><li><span><a href=\"#The-“Deep”-in-Deep-Reinforcement-Learning\" data-toc-modified-id=\"The-“Deep”-in-Deep-Reinforcement-Learning-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>The “Deep” in Deep Reinforcement Learning</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Glossary\" data-toc-modified-id=\"Glossary-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Glossary</a></span></li><li><span><a href=\"#Hands-on\" data-toc-modified-id=\"Hands-on-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Hands-on</a></span></li><li><span><a href=\"#Quiz\" data-toc-modified-id=\"Quiz-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Quiz</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Additional-Readings\" data-toc-modified-id=\"Additional-Readings-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Additional Readings</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a6ee8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e48aab2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/what-is-rl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad808e30",
   "metadata": {},
   "source": [
    "Deep RL is a type of Machine Learning where an agent learns <b>how to behave</b> in an environment <b>by performing actions and seeing the results</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5bf811",
   "metadata": {},
   "source": [
    "# What is Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631d1ae",
   "metadata": {},
   "source": [
    "## The big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08db4a3",
   "metadata": {},
   "source": [
    "### The formal definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d030d72",
   "metadata": {},
   "source": [
    "> Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b0cfb",
   "metadata": {},
   "source": [
    "# The Reinforcement Learning Framework\n",
    "\n",
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/rl-framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49683164",
   "metadata": {},
   "source": [
    "## The RL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adfa6c",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" style=\"width:600px;\" title=\"RL process\">\n",
    "\n",
    "A loop of state, action, reward, and next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77be6e",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" style=\"width:300px;\" title=\"RL loops outputs sequuence\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80c85e",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size: large\">The agent’s goal is to <i>maximize</i> its cumulative reward, called the <b>expected return</b>.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086bd0c",
   "metadata": {},
   "source": [
    "## The reward hypothesis: the central idea of reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf31abd",
   "metadata": {},
   "source": [
    "## Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de84f36",
   "metadata": {},
   "source": [
    "In papers, you’ll see that the RL process is called a <b>Markov Decision Process</b> (MDP).\n",
    "\n",
    "The Markov Property implies that our agent needs <b>only the current state to decide</b> what action to take and <b>not the history of all the states and actions</b> they took before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8823aad",
   "metadata": {},
   "source": [
    "## Observations/States Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7552f",
   "metadata": {},
   "source": [
    "Observations/States are the <b>information our agent gets from the environment</b>. For e.g., frame in a video game, value of certain stock in trading.\n",
    "\n",
    "- State $s$: Fully observed environment\n",
    "    - Chess\n",
    "- Observation $o$: Partially observed environment\n",
    "    - Super mario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af40a7",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg\" style=\"width:600px;\" title=\"Observation vs State space\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa99444",
   "metadata": {},
   "source": [
    "## Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac98cc1",
   "metadata": {},
   "source": [
    "The Action space is the set of <b>all possible actions in an environment</b>.\n",
    "\n",
    "- Discrete space: the number of possible actions is finite.\n",
    "    - e.g., Super mario\n",
    "- Continuous space: the number of possible actions is infinite.\n",
    "    - e.g., Self Driving car\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg\" style=\"width:600px;\" title=\"Action space\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3081f6",
   "metadata": {},
   "source": [
    "## Rewards and the discounting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5562eb25",
   "metadata": {},
   "source": [
    "<i>Cumulative reward = Sum of all rewards in the sequence<i>\n",
    "    \n",
    "The cumulative reward at each time step $t$ can be written as:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg\" style=\"width:500px;\" title=\"Cumulative reward\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg\" style=\"width:200px;\" title=\"Cumulative reward\">\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaad9e9",
   "metadata": {},
   "source": [
    "However, in reality, <b>we can’t just add them like that</b>. The rewards that come sooner (at the beginning of the game) <b>are more likely to happen</b> since they are more predictable than the long-term future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f8176",
   "metadata": {},
   "source": [
    "Our discounted expected cumulative reward is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da3715",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\" style=\"width:500px;\" title=\"Discounted exepcted cumulative reward\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a57136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T01:39:06.583985Z",
     "start_time": "2023-08-12T01:39:06.571697Z"
    }
   },
   "source": [
    "where\n",
    "- $\\gamma$ is discount rate, <b>between 0 and 1</b>\n",
    "    - Larger the $\\gamma$, smaller the discount => Agent <b>cares more about long-term reward.</b>\n",
    "    - Smaller the $\\gamma$, larger the discount => Agent <b>cares more abotu the short-term reward.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0939e334",
   "metadata": {},
   "source": [
    "# The type of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0ee53",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/tasks\n",
    "\n",
    "A task is an <b>instance</b> of a Reinforcement Learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d586bb",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/tasks.jpg\" style=\"width:600px;\" title=\"Type of tasks\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7616ce6",
   "metadata": {},
   "source": [
    "## Episodic task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2011bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T01:43:52.172604Z",
     "start_time": "2023-08-12T01:43:52.163282Z"
    }
   },
   "source": [
    "- e.g., Super Mario Bros\n",
    "    - An episode begins at launch of a new level and ends <b>when we are killed or reach end of the level<b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc38df",
   "metadata": {},
   "source": [
    "## Continuous task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2517df",
   "metadata": {},
   "source": [
    "- e.g., Automated Stock trading\n",
    "    - There's no starting point and ending point $\\Rightarrow$ No terminal state. <b>The agent keeps running till we decide to stop it.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8e6dd",
   "metadata": {},
   "source": [
    "# The Exploration/ Exploitation tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a50de",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/exp-exp-tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cb681",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg\" style=\"width:600px;\" title=\"Exploration Exploitation Tradeoff\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f88fdf",
   "metadata": {},
   "source": [
    "We need to balance how much we <b>explore the environment</b> and how much we <b>exploit what we know about the environment</b>.\n",
    "\n",
    "Therefore, we must <b>define a rule that helps to handle this trade-off</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8141c",
   "metadata": {},
   "source": [
    "# The two main approaches for solving RL problems\n",
    "\n",
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/two-methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f00fb3",
   "metadata": {},
   "source": [
    "## The policy π: the agent’s brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af63dc7",
   "metadata": {},
   "source": [
    "The Policy <b>π</b> is the <b>brain of our Agent</b>, it’s the <i>function that tells us what <b>action to take given the state</b></i>. So it <b>defines the agent’s behavior</b> at a given time.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_1.jpg\" style=\"width:300px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a485e8",
   "metadata": {},
   "source": [
    "This Policy is the <b>function we want to learn</b>, our goal is to find the optimal policy π*, the policy that <b>maximizes expected return</b> when the agent acts according to it. <b>We find this optimal policy π* through training.</b>\n",
    "\n",
    "There are two approaches to train our agent to find this optimal policy π*:\n",
    "- <b>Policy-Based Methods</b>: <b>Directly</b>, by teaching the agent to <b>learn which action to take</b>, given the current state\n",
    "- <b>Value-Based Methods</b>: <b>Indirectly</b>, teach the agent to <b>learn which state is more valuable</b> and then take the <b>action that leads to the more valuable states</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e22fa7",
   "metadata": {},
   "source": [
    "| Policy-based | Value-based | \n",
    "| :-: | :-: |\n",
    "| We <b>learn policy function directly</b> | Instead of learning a policy function, we <b>learn a value function</b> that maps a state to the exepcted value <b>of being at the state</b> |\n",
    "| <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_1.jpg\" style=\"width:450px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\"> <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_2.jpg\" style=\"width:450px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\"> |  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_1.jpg\" style=\"width:450px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\"> <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_2.jpg\" style=\"width:450px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580cfb5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T01:48:59.040231Z",
     "start_time": "2023-08-12T01:48:59.032518Z"
    }
   },
   "source": [
    "## Policy-Based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255420ee",
   "metadata": {},
   "source": [
    "Two types of policies:\n",
    "1. <i>Deterministic</i>: a policy at a given state <b>will always return the same action</b>.\n",
    "    - $a=\\pi (s)$ where\n",
    "        - $pi$ is the policy, brain of our agent, the function we want to learn\n",
    "        - $s$ is the state\n",
    "        - $a$ is action for given state\n",
    "2. <i>Stochastic</i>: outputs a <b>probability distribution over actions</b>.\n",
    "    - $\\pi (a|s) = P[A|s]$ where\n",
    "        - $P[A|s]$ is the probability distribution over the set of actions given the state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb8c22",
   "metadata": {},
   "source": [
    "## Value-Based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112648bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8af24795",
   "metadata": {},
   "source": [
    "# The “Deep” in Deep Reinforcement Learning\n",
    "\n",
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/deep-rl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95740361",
   "metadata": {},
   "source": [
    "Deep Reinforcement Learning introduces <b>deep neural networks to solve Reinforcement Learning problems</b> — hence the name “deep”.\n",
    "\n",
    "Two value-based algorithms: \n",
    "- <i>Q-Learning (classic Reinforcement Learning)</i>: <b>Use a traditional algorithm</b> to create a Q table that helps us to find the action to take for each state\n",
    "- <i>Deep Q-Learning</i>: <b>Use a Neural network</b> (to approximate Q value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc9343",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" style=\"width:700px;\" title=\"Q learning vs. Deep Q learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466c4f8",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd166def",
   "metadata": {},
   "source": [
    "- Reinforcement Learning is a computational approach of learning from actions. We build an agent that learns from the environment <b>by interacting with it through trial and error</b> and receiving rewards (negative or positive) as feedback.\n",
    "\n",
    "- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the <b>reward hypothesis</b>, which is that <b>all goals can be described as the maximization of the expected cumulative reward</b>.\n",
    "\n",
    "- The RL process is a loop that outputs a sequence of <b>state, action, reward and next state</b>.\n",
    "\n",
    "- To calculate the expected cumulative reward (expected return), we discount the rewards: <b>the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long term future reward.</b>\n",
    "\n",
    "- To solve an RL problem, you want to <b>find an optimal policy</b>. The policy is the “brain” of your agent, which will tell us <b>what action to take given a state</b>. The optimal policy is the one which <b>gives you the actions that maximize the expected return.</b>There are two ways to find your optimal policy:\n",
    "    - By training your policy directly: <b>policy-based methods.</b>\n",
    "    - By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: <b>value-based methods.</b>\n",
    "\n",
    "- Finally, we speak about Deep RL because we introduce <b>deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based)</b> hence the name “deep”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9ad1d",
   "metadata": {},
   "source": [
    "# Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81194521",
   "metadata": {},
   "source": [
    "<b>Agent</b>:\n",
    "An agent learns to <b>make decisions by trial and error, with rewards and punishments from the surroundings</b>.\n",
    "\n",
    "<b>Environment</b>:\n",
    "An environment is a simulated world where an <b>agent can learn by interacting with it</b>.\n",
    "\n",
    "<b>Markov Property</b>:\n",
    "It implies that the action taken by our agent is <b>conditional solely on the present state and independent of the past states and actions.</b>\n",
    "\n",
    "<b>Observations/State</b>\n",
    "- <b>State</b>: Complete description of the state of the world.\n",
    "- <b>Observation</b>: Partial description of the state of the environment/world.\n",
    "\n",
    "<b>Actions</b>\n",
    "- <b>Discrete Actions</b>: Finite number of actions, such as left, right, up, and down.\n",
    "- <b>Continuous Actions</b>: Infinite possibility of actions; for example, in the case of self-driving cars, the driving scenario has an infinite possibility of actions occurring.\n",
    "\n",
    "<b>Rewards and Discounting</b>\n",
    "- <b>Rewards</b>: Fundamental factor in RL. Tells the agent whether the action taken is good/bad.\n",
    "- RL algorithms are focused on maximizing the <b>cumulative reward</b>.\n",
    "- <b>Reward Hypothesis</b>: RL problems can be formulated as a maximisation of (cumulative) return.\n",
    "- <b>Discounting</b> is performed because rewards obtained at the start are more likely to happen as they are more predictable than long-term rewards.\n",
    "\n",
    "<b>Tasks</b>\n",
    "- <b>Episodic</b>: Has a starting point and an ending point.\n",
    "- <b>Continuous</b>: Has a starting point but no ending point.\n",
    "\n",
    "<b>Exploration v/s Exploitation Trade-Off</b>\n",
    "- <b>Exploration</b>: It’s all about exploring the environment by trying random actions and receiving feedback/returns/rewards from the environment.\n",
    "- <b>Exploitation</b>: It’s about exploiting what we know about the environment to gain maximum rewards.\n",
    "- <b>Exploration-Exploitation Trade-Off</b>: It balances how much we want to <b>explore</b> the environment and how much we want to <b>exploit</b> what we know about the environment.\n",
    "\n",
    "<b>Policy</b>\n",
    "- <b>Policy</b>: It is called the <b>agent’s brain</b>. It tells us <b>what action to take, given the state</b>.\n",
    "- <b>Optimal Policy</b>: Policy that <b>maximizes the expected return</b> when an agent acts according to it. It is learned through training.\n",
    "- Two types\n",
    "    - <b>Policy-based Methods:</b>\n",
    "        - An approach to solving RL problems.\n",
    "        - In this method, the Policy is learned directly.\n",
    "        - Will map each state to the best corresponding action at that state. Or a probability distribution over the set of possible actions at that state.\n",
    "\n",
    "    - <b>Value-based Methods:</b>\n",
    "        - Another approach to solving RL problems.\n",
    "        - Here, instead of training a policy, we train a value function that maps each state to the expected value of being in that state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a2b3e",
   "metadata": {},
   "source": [
    "# Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc8233",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf51fc1",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312834d3",
   "metadata": {},
   "source": [
    "# Additional Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f0e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
