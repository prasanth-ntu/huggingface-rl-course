{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a06359b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#What-is-RL?-A-short-recap\" data-toc-modified-id=\"What-is-RL?-A-short-recap-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What is RL? A short recap</a></span></li><li><span><a href=\"#The-two-types-of-value-based-methods\" data-toc-modified-id=\"The-two-types-of-value-based-methods-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The two types of value-based methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-state-value-function,-$V$\" data-toc-modified-id=\"The-state-value-function,-$V$-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>The state-value function, $V$</a></span></li><li><span><a href=\"#The-action-value-function,-$Q$\" data-toc-modified-id=\"The-action-value-function,-$Q$-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The action-value function, $Q$</a></span></li></ul></li><li><span><a href=\"#The-Bellman-Equation,-simplify-our-value-estimation\" data-toc-modified-id=\"The-Bellman-Equation,-simplify-our-value-estimation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>The Bellman Equation, simplify our value estimation</a></span></li><li><span><a href=\"#Monte-Carlo-vs-Temporal-Difference-Learning\" data-toc-modified-id=\"Monte-Carlo-vs-Temporal-Difference-Learning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Monte Carlo vs Temporal Difference Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Monte-Carlo:-learning-at-the-end-of-the-episode\" data-toc-modified-id=\"Monte-Carlo:-learning-at-the-end-of-the-episode-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Monte Carlo: learning at the end of the episode</a></span></li><li><span><a href=\"#Temporal-Difference-Learning:-learning-at-each-step\" data-toc-modified-id=\"Temporal-Difference-Learning:-learning-at-each-step-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Temporal Difference Learning: learning at each step</a></span></li></ul></li><li><span><a href=\"#Mid-way-Recap\" data-toc-modified-id=\"Mid-way-Recap-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Mid-way Recap</a></span></li><li><span><a href=\"#Mid-way-Quiz\" data-toc-modified-id=\"Mid-way-Quiz-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Mid-way Quiz</a></span></li><li><span><a href=\"#Introducing-Q-Learning\" data-toc-modified-id=\"Introducing-Q-Learning-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Introducing Q-Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-Q-Learning?\" data-toc-modified-id=\"What-is-Q-Learning?-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>What is Q-Learning?</a></span></li><li><span><a href=\"#The-Q-Learning-algorithm\" data-toc-modified-id=\"The-Q-Learning-algorithm-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>The Q-Learning algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-We-initialize-the-Q-table\" data-toc-modified-id=\"Step-1:-We-initialize-the-Q-table-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span>Step 1: We initialize the Q-table</a></span></li><li><span><a href=\"#Step-2:-Choose-an-action-using-the-epsilon-greedy-strategy\" data-toc-modified-id=\"Step-2:-Choose-an-action-using-the-epsilon-greedy-strategy-8.2.2\"><span class=\"toc-item-num\">8.2.2&nbsp;&nbsp;</span>Step 2: Choose an action using the epsilon-greedy strategy</a></span></li><li><span><a href=\"#Step-3:-Perform-action-At,-get-reward-Rt+1-and-next-state-St+1\" data-toc-modified-id=\"Step-3:-Perform-action-At,-get-reward-Rt+1-and-next-state-St+1-8.2.3\"><span class=\"toc-item-num\">8.2.3&nbsp;&nbsp;</span>Step 3: Perform action At, get reward Rt+1 and next state St+1</a></span></li><li><span><a href=\"#Step-4:-Update-Q(St,-At)\" data-toc-modified-id=\"Step-4:-Update-Q(St,-At)-8.2.4\"><span class=\"toc-item-num\">8.2.4&nbsp;&nbsp;</span>Step 4: Update Q(St, At)</a></span></li></ul></li><li><span><a href=\"#Off-policy-vs-On-policy\" data-toc-modified-id=\"Off-policy-vs-On-policy-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Off-policy vs On-policy</a></span></li></ul></li><li><span><a href=\"#A-Q-Learning-example\" data-toc-modified-id=\"A-Q-Learning-example-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>A Q-Learning example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Initialize-the-Q-table\" data-toc-modified-id=\"Step-1:-Initialize-the-Q-table-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Step 1: Initialize the Q-table</a></span></li><li><span><a href=\"#Step-2:-Choose-an-action-using-the-Epsilon-Greedy-Strategy\" data-toc-modified-id=\"Step-2:-Choose-an-action-using-the-Epsilon-Greedy-Strategy-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Step 2: Choose an action using the Epsilon Greedy Strategy</a></span></li><li><span><a href=\"#Step-3:-Perform-action-At,-get-Rt+1-and-St+1\" data-toc-modified-id=\"Step-3:-Perform-action-At,-get-Rt+1-and-St+1-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Step 3: Perform action At, get Rt+1 and St+1</a></span></li><li><span><a href=\"#Step-4:-Update-Q(St,-At)\" data-toc-modified-id=\"Step-4:-Update-Q(St,-At)-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Step 4: Update Q(St, At)</a></span></li><li><span><a href=\"#Step-2:-Choose-an-action-using-the-Epsilon-Greedy-Strategy\" data-toc-modified-id=\"Step-2:-Choose-an-action-using-the-Epsilon-Greedy-Strategy-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>Step 2: Choose an action using the Epsilon Greedy Strategy</a></span></li><li><span><a href=\"#Step-3:-Perform-action-At,-get-Rt+1-and-St+1\" data-toc-modified-id=\"Step-3:-Perform-action-At,-get-Rt+1-and-St+1-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>Step 3: Perform action At, get Rt+1 and St+1</a></span></li><li><span><a href=\"#Step-4:-Update-Q(St,-At)\" data-toc-modified-id=\"Step-4:-Update-Q(St,-At)-9.7\"><span class=\"toc-item-num\">9.7&nbsp;&nbsp;</span>Step 4: Update Q(St, At)</a></span></li></ul></li><li><span><a href=\"#Q-Learning-Recap\" data-toc-modified-id=\"Q-Learning-Recap-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Q-Learning Recap</a></span></li><li><span><a href=\"#Glossary\" data-toc-modified-id=\"Glossary-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Glossary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Strategies-to-find-the-optimal-policy\" data-toc-modified-id=\"Strategies-to-find-the-optimal-policy-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Strategies to find the optimal policy</a></span></li><li><span><a href=\"#Epsilon-greedy-strategy:\" data-toc-modified-id=\"Epsilon-greedy-strategy:-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Epsilon-greedy strategy:</a></span></li><li><span><a href=\"#Greedy-strategy:\" data-toc-modified-id=\"Greedy-strategy:-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>Greedy strategy:</a></span></li><li><span><a href=\"#Off-policy-vs-on-policy-algorithms\" data-toc-modified-id=\"Off-policy-vs-on-policy-algorithms-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;</span>Off-policy vs on-policy algorithms</a></span></li><li><span><a href=\"#Monte-Carlo-and-Temporal-Difference-learning-strategies\" data-toc-modified-id=\"Monte-Carlo-and-Temporal-Difference-learning-strategies-11.5\"><span class=\"toc-item-num\">11.5&nbsp;&nbsp;</span>Monte Carlo and Temporal Difference learning strategies</a></span></li></ul></li><li><span><a href=\"#Hands-on\" data-toc-modified-id=\"Hands-on-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Hands-on</a></span><ul class=\"toc-item\"><li><span><a href=\"#q-Frozenlake-v1-4x4-noSlippery\" data-toc-modified-id=\"q-Frozenlake-v1-4x4-noSlippery-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>q-Frozenlake-v1-4x4-noSlippery</a></span></li><li><span><a href=\"#q-Taxi-v3\" data-toc-modified-id=\"q-Taxi-v3-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>q-Taxi-v3</a></span></li></ul></li><li><span><a href=\"#Q-Learning-Quiz\" data-toc-modified-id=\"Q-Learning-Quiz-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Q-Learning Quiz</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Additional-Readings\" data-toc-modified-id=\"Additional-Readings-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Additional Readings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Monte-Carlo-and-TD-Learning\" data-toc-modified-id=\"Monte-Carlo-and-TD-Learning-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>Monte Carlo and TD Learning</a></span></li><li><span><a href=\"#Q-Learning\" data-toc-modified-id=\"Q-Learning-15.2\"><span class=\"toc-item-num\">15.2&nbsp;&nbsp;</span>Q-Learning</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d20de",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe2d837",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/deep-rl-course/unit2/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87091e7b",
   "metadata": {},
   "source": [
    "In the first unit of this class, we learned about Reinforcement Learning (RL), the RL process, and the different methods to solve an RL problem. We also <b>trained our first agents and uploaded them to the Hugging Face Hub</b>.\n",
    "\n",
    "In this unit, we’re going to <b>dive deeper into one of the Reinforcement Learning methods: value-based methods</b> and study our <b>first RL algorithm: Q-Learning.</b>\n",
    "\n",
    "We’ll also <b>implement our first RL agent from scratch</b>, a Q-Learning agent, and will train it in two environments:\n",
    "- Frozen-Lake-v1 (non-slippery version): where our agent will need to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoiding holes (H).\n",
    "- An autonomous taxi: where our agent will need to learn to navigate a city to transport its passengers from point A to point B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d835e",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" style=\"width:700px;\" title=\"Two environments\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b87ad",
   "metadata": {},
   "source": [
    "Concretely, we will:\n",
    "\n",
    "- Learn about <b>value-based methods</b>.\n",
    "- Learn about the <b>differences between Monte Carlo and Temporal Difference Learning</b>.\n",
    "- Study and implement <b>our first RL algorithm</b>: Q-Learning.\n",
    "\n",
    "This unit is <b>fundamental if you want to be able to work on Deep Q-Learning</b>: the first Deep RL algorithm that played Atari games and beat the human level on some of them (breakout, space invaders, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8da45",
   "metadata": {},
   "source": [
    "# What is RL? A short recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e9367",
   "metadata": {},
   "source": [
    "In RL, we build an agent that can <b>make smart decisions</b>. For instance, an agent that <b>learns to play a video game</b>. Or a trading agent that <b>learns to maximize its benefits</b> by deciding on <b>what stocks to buy and when to sell</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa633ef2",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/rl-process.jpg\" style=\"width:700px;\" title=\"RL Process\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c184c",
   "metadata": {},
   "source": [
    "To make intelligent decisions, our agent will learn from the environment by <b>interacting with it through trial and error and</b> receiving rewards (positive or negative) as <b>unique feedback</b>.\n",
    "\n",
    "Its goal is to <b>maximize its expected cumulative reward</b> (because of the reward hypothesis).\n",
    "\n",
    "The <b>agent’s decision-making process is called the policy π</b>: given a state, a policy will output an action or a probability distribution over actions. That is, given an observation of the environment, a policy will provide an action (or multiple probabilities for each action) that the agent should take.\n",
    "\n",
    "<img src=\"\n",
    "https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/policy.jpg\" style=\"width:700px;\" title=\"Policy: The agent's brain\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e038f5",
   "metadata": {},
   "source": [
    "Our <b>goal is to find an optimal policy π*</b>, aka., a policy that leads to the best expected cumulative reward.\n",
    "\n",
    "And to find this optimal policy (hence solving the RL problem), there are <b>two main types of RL methods</b>:\n",
    "\n",
    "- <i>Policy-based methods</i>: <b>Train the policy directly</b> to learn which action to take given a state.\n",
    "- <i>Value-based methods</i>: <b>Train a value function</b> to learn <b>which state is more valuable</b> and use this value function to <b>take the action that leads to it</b>.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg\" style=\"width:700px;\" title=\"RL Process\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38896f",
   "metadata": {},
   "source": [
    "# The two types of value-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6facb",
   "metadata": {},
   "source": [
    "In value-based methods, <b>we learn a value function that maps a state to the expected value of being at that state.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b72194",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" style=\"width:700px;\" title=\"Value-based Methods\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c541655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T08:26:01.860464Z",
     "start_time": "2023-08-17T08:26:01.847788Z"
    }
   },
   "source": [
    "<span style=\"font-size:large\">$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left [R_{t+1} + \\gamma R_{t+2} + \\gamma R_{t+3} + \\cdots | S_{t} = s \\right ]$$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43623c",
   "metadata": {},
   "source": [
    "The value of a state is the <b>expected discounted return</b> the agent can get if it <b>starts at that state and then acts according to our policy.</b>\n",
    "\n",
    "> But what does it mean to act according to our policy? After all, we don't have a policy in value-based methods since we train a value function and not a policy.\n",
    "\n",
    "Remember that the goal of an <b>RL agent is to have an optimal policy π*.</b>\n",
    "\n",
    "To find the optimal policy, we learned about two different methods:\n",
    "\n",
    "- <i>Policy-based methods</i>: <b>Directly train the policy</b> to select what action to take given a state (or a probability distribution over actions at that state). In this case, we <b>don’t have a value function.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789acc6",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-2.jpg\" style=\"width:700px;\" title=\"Two approaches to find optimal policy: policy based methods\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a63eb",
   "metadata": {},
   "source": [
    "- <i>Policy-based methods</i>: The policy takes a state as input and outputs what action to take at that state (deterministic policy: a policy that output one action given a state, contrary to stochastic policy that output a probability distribution over actions).\n",
    "\n",
    "And consequently, <b>we don’t define by hand the behavior of our policy; it’s the training that will define it.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa132bf",
   "metadata": {},
   "source": [
    "- <i>Value-based methods</i>: <b>Indirectly, by training a value function</b> that outputs the value of a state or a state-action pair. Given this value function, our policy <b>will take an action.<b/>\n",
    "\n",
    "Since the policy is not trained/learned, <b>we need to specify its behavior</b>. For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward, <span style=\"color:blue\"><b>we’ll create a Greedy Policy.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e445345",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-3.jpg\" style=\"width:700px;\" title=\"Two approaches to find optimal policy: value based methods\">\n",
    "<span style=\"font-size:small; color:blue\"><i>Given a state, our action-value function (that we train) outputs the value of each action at that state. Then, our pre-defined Greedy Policy selects the action that will yield the highest value given a state or a state action pair.</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacfb92d",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Consequently, whatever method you use to solve your problem, <b>you will have a policy</b></span>. In the case of value-based methods, you don’t train the policy: your policy <b>is just a simple pre-specified function</b> (for instance, the Greedy Policy) that uses the values given by the value-function to select its actions.\n",
    "\n",
    "<b>So the difference is:</b>\n",
    "- <span style=\"color:blue\">In policy-based training, <b>the optimal policy (denoted π*) is found by training the policy directly.</b></span>\n",
    "- <span style=\"color:blue\">In value-based training, <b>finding an optimal value function (denoted Q* or V*, we’ll study the difference below) leads to having an optimal policy.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7a1e7",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" style=\"width:500px;\" title=\"The link between value and policy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3066d3",
   "metadata": {},
   "source": [
    "<span style=\"font-size:large\">$$\\pi^{*}(s) = arg \\, \\underset{a}{max} \\, Q^{*}(s,a)$$ </span>\n",
    "where\n",
    "- $\\pi^{*}$ is the optimal policy.\n",
    "- $Q^{*}$ is the optimal value function.\n",
    "- $\\underset{a}{max}$ is the pre-defined Greedy Policy that selects the action that will yield the highest value given a state or state action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b94cc7",
   "metadata": {},
   "source": [
    "In fact, most of the time, in value-based methods, you’ll use <span style=\"color:blue\"><b>an Epsilon-Greedy Policy</b></span> that handles the exploration/exploitation trade-off; we’ll talk about this when we talk about Q-Learning in the second part of this unit.\n",
    "\n",
    "As we mentioned above, we have two types of value-based functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b14bf9",
   "metadata": {},
   "source": [
    "## The state-value function, $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb994f",
   "metadata": {},
   "source": [
    "We write the state value function under a policy π like this:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-1.jpg\" style=\"width:700px;\" title=\"State value function\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65534b0",
   "metadata": {},
   "source": [
    "<span style=\"font-size:large\">$$V_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left [ G_{t} | S_{t}=s \\right ]$$ </span>\n",
    "where\n",
    "- $V_{\\pi}(s)$ is the value of the state, $s$.\n",
    "- $\\mathbb{E}_{\\pi}$ is the expected return if the $\\cdots$.\n",
    "- $S_{t}=s$ if the agent starts at state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeea215",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">For each state, the state-value function outputs the expected return if the agent <b>starts at that state</b> and then follows the policy forever afterward (for all future timesteps, if you prefer).</span>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-2.jpg\" style=\"width:700px;\" title=\"State value function\">\n",
    "<span style=\"font-size:small\"><i>If we take the state with value -7: it's the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right.</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ede6f9",
   "metadata": {},
   "source": [
    "## The action-value function, $Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6610c",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">In the action-value function, for each state and action pair, the action-value function <b>outputs the expected return</b> if the agent starts in that state, takes that action, and then follows the policy forever after.</span>\n",
    "\n",
    "The value of taking action $a$ in state $s$  under a policy $/pi$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9c97a",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-1.jpg\" style=\"width:700px;\" title=\"Action State value function\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208c8ea",
   "metadata": {},
   "source": [
    "<span style=\"font-size:large\">$$Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\left [ G_{t} | S_{t}=s, A_{t}=a \\right ]$$ </span>\n",
    "where\n",
    "- $Q_{\\pi}(s, a)$ is the value of the state-action pair, $s$.\n",
    "- $\\mathbb{E}_{\\pi}$ is the expected return if the $\\cdots$.\n",
    "- $S_{t}=s$ agent starts at state $s$.\n",
    "- $A_{t}=a$ and agent chooses action $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12edee8",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-2.jpg\" style=\"width:700px;\" title=\"Action State value function\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc241442",
   "metadata": {},
   "source": [
    "<b>We see that the difference is:</b>\n",
    "- <span style=\"color:blue\">For the state-value function, we calculate <b>the value of a state $S_{t}$</b></span>\n",
    "- <span style=\"color:blue\">For the action-value function, we calculate <b>the value of the state-action pair ($S_{t}$, $A_{t}$) hence the value of taking that action at that state.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346f5f1",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-types.jpg\" style=\"width:700px;\" title=\"Two types of value function\">\n",
    "<span style=\"font-size:small\"><i>Note: We didn't fill all the state-action pairs for the example of Action-value function</i></span>\n",
    "\n",
    "—In either case, whichever value function we choose (state-value or action-value function), <b>the returned value is the expected return.</b>\n",
    "\n",
    "<span style=\"color:red\">However, the problem is that <b>to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.</b>This can be a computationally expensive process,</span> and that’s <b>where the Bellman equation comes in to help us.</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a827283",
   "metadata": {},
   "source": [
    "# The Bellman Equation, simplify our value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5cc54",
   "metadata": {},
   "source": [
    "The Bellman equation simplifies our state value or state-action value calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a27cba",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman.jpg\" style=\"width:700px;\" title=\"Bellman equation\">\n",
    "\n",
    "With what we have learned so far, we know that if we calculate $V(S_{t})$ (the value of a state), we need to calculate the return starting at that state and then follow the policy forever after. <b>(The policy we defined in the following example is a Greedy Policy; for simplification, we don’t discount the reward).</b>\n",
    "\n",
    "So to calculate $V(S_{t})$, we need to calculate the sum of the expected rewards. Hence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5214c3",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg\" style=\"width:700px;\" title=\"Bellman equation\">\n",
    "<span style=\"font-size:small\"><i>To calculate the value of State 1: the sum of rewards if the agent started in that state and then followed the greedy policy (taking actions that leads to the best states values) for all the time steps.</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa1911",
   "metadata": {},
   "source": [
    "Then, to calculate the $V(S_{t+1})$ we need to calculate the return starting at that state $S_{t+1}$.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman3.jpg\" style=\"width:700px;\" title=\"Bellman equation\">\n",
    "<span style=\"font-size:small\"><i>To calculate the value of State 2: the sum of rewards <b>if the agent started in that state</b>, and then followed the <b>policy for all the time steps</b>.</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ef0fee",
   "metadata": {},
   "source": [
    "So you may have noticed, we’re repeating the computation of the value of different states, which can be tedious if you need to do it for each state value or state-action value.\n",
    "\n",
    "<span style=\"color:blue\">Instead of calculating the expected return for each state or each state-action pair, <b>we can use the Bellman equation</b>.</span> (hint: if you know what Dynamic Programming is, this is very similar! if you don’t know what it is, no worries!)\n",
    "\n",
    "The Bellman equation is a recursive equation that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as:\n",
    "\n",
    "<b>The immediate reward $R_{t+1}$ $+$ the discounted value of the state that follows $(\\gamma * V(S_{t+1}))$.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd8eda5",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg\" style=\"width:700px;\" title=\"Bellman equation\">\n",
    "\n",
    "If we go back to our example, we can say that the value of State 1 is equal to the expected cumulative return if we start at that state.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman5.jpg\" style=\"width:700px;\" title=\"Bellman equation\">\n",
    "\n",
    "To calculate the value of State 1: the sum of rewards <b>if the agent started in that state 1</b> and then followed the <b>policy for all the time steps</b>.\n",
    "\n",
    "This is equivalent to \n",
    "\n",
    "<span style=\"font-size:large\">$V(S_{t})$ = Immediate reward $R_{t+1}$ + Discounted value of the next state $(\\gamma * V(S_{t+1}))$.</span>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman6.jpg\" style=\"width:700px;\" title=\"Bellman equation\">\n",
    "<span style=\"font-size:small\"><i>\n",
    "For simplification, here we don’t discount so gamma = 1.</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f1209",
   "metadata": {},
   "source": [
    "In the interest of simplicity, here we don’t discount, so $\\gamma$ = 1. But you’ll study an example with $\\gamma$ = 0.99 in the Q-Learning section of this unit.\n",
    "\n",
    "- The value of $V(S_{t+1})$ = Immediate reward $R_{t+2}$ + Discounted value of the next state  $(\\gamma * V(S_{t+2}))$.\n",
    "- And so on.\n",
    "\n",
    "<span style=\"color:blue\">To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, <b>which is a long process</b>, we calculate the value as the <b>sum of immediate reward + the discounted value of the state that follows.</b></span>\n",
    "\n",
    "Before going to the next section, think about the role of gamma in the Bellman equation. What happens if the value of gamma is very low (e.g. 0.1 or even 0)? What happens if the value is 1? What happens if the value is very high, such as a million?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5b23c",
   "metadata": {},
   "source": [
    "# Monte Carlo vs Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755c4f4",
   "metadata": {},
   "source": [
    "The last thing we need to discuss before diving into Q-Learning is the two learning strategies.\n",
    "\n",
    "Remember that an RL agent <b>learns by interacting with its environment</b>. The idea is that <b>given the experience and the received reward, the agent will update its value function or policy.</b>\n",
    "\n",
    "Monte Carlo and Temporal Difference Learning are two different <b>strategies on how to train our value function or our policy function</b>. Both of them <b>use experience to solve the RL problem.</b>\n",
    "\n",
    "<span style=\"color:blue\">On one hand, Monte Carlo uses <b>an entire episode of experience before learning</b>. On the other hand, Temporal Difference uses <b>only a step $(S_{t}, A_{t}, R_{t+1}, S_{t+1})$ to learn.</span>\n",
    "\n",
    "We’ll explain both of them <b>using a value-based method example.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f876e",
   "metadata": {},
   "source": [
    "## Monte Carlo: learning at the end of the episode\n",
    "\n",
    "Monte Carlo waits until the end of the episode, calculates $G_{t}$ return) and uses it as a <b>target for updating $V_(S_{t})$.</b>\n",
    "\n",
    "So it requires a <b>complete episode of interaction before updating our value function.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb9f35",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg\" style=\"width:700px;\" title=\"Monte Carlo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13734b51",
   "metadata": {},
   "source": [
    "<span style=\"font-size:large\">$$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha[G_{t} - V(S_{t})]$$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5521fc5",
   "metadata": {},
   "source": [
    "If we take an example:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-2.jpg\" style=\"width:700px;\" title=\"Monte Carlo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabd7d9",
   "metadata": {},
   "source": [
    "- We always start the episode <b>at the same starting point.</b>\n",
    "- <b>The agent takes actions using the policy</b>. For instance, using an Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation.\n",
    "- We get the <b>reward and the next state.</b>\n",
    "- We terminate the episode if the cat eats the mouse or if the mouse moves > 10 steps.\n",
    "- At the end of the episode, <b>we have a list of State, Actions, Rewards, and Next States tuples</b>. \n",
    "    - For instance \n",
    "    ```\n",
    "    [\n",
    "        [State tile 3 bottom, Go Left, +1, State tile 2 bottom], \n",
    "        [State tile 2 bottom, Go Left, +0, State tile 1 bottom],\n",
    "        …\n",
    "    ]```\n",
    "- <b>The agent will sum the total rewards $G_{t}$</b> (to see how well it did).\n",
    "- It will then update $V(s_{t})$ based on the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec8454",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3.jpg\" style=\"width:500px;\" title=\"Monte Carlo\">\n",
    "\n",
    "- Then start a new game with this new knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832ebc8",
   "metadata": {},
   "source": [
    "By running more and more episodes, <b>the agent will learn to play better and better.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg\" style=\"width:700px;\" title=\"Monte Carlo\">\n",
    "\n",
    "For instance, if we train a state-value function using Monte Carlo:\n",
    "- We initialize our value function <b>so that it returns 0 value for each state</b>\n",
    "- Our learning rate (lr) is 0.1 and our discount rate is 1 (= no discount)\n",
    "- Our mouse <b>explores the environment and takes random actions</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b168d",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4.jpg\" style=\"width:700px;\" title=\"Monte Carlo\">\n",
    "- The mouse made more than 10 steps, so the episode ends .\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4p.jpg\" style=\"width:700px;\" title=\"Monte Carlo\">\n",
    "\n",
    "- We have a list of state, action, rewards, next_state, <b>we need to calculate the return $G_{t}=0$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e77ce",
   "metadata": {},
   "source": [
    "$G_{t} = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots$ (for simplicity, we don’t discount the rewards)\n",
    "\n",
    "$G_{0} = R_{1} + R_{2} + R_{3} + \\cdots$\n",
    "\n",
    "$G_{0} = 1 + 0 + 0 + 0 + 0 + 0 + 1 +1 + 0 + 0$\n",
    "\n",
    "$G_{0} = 3$\n",
    "\n",
    "- We can now compute the new $V(S_{0})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db16d52",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5.jpg\" style=\"width:600px;\" title=\"Monte Carlo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65628e",
   "metadata": {},
   "source": [
    "$V(S_{0}) = V(S_{0}) + lr * [G_{0} - V(S_{0})]$\n",
    "\n",
    "$V(S_{0}) = 0 + 0.1 * [3-0]$\n",
    "\n",
    "$V(S_{0}) = 0.3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a7291",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5p.jpg\" style=\"width:700px;\" title=\"Monte Carlo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2e971",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning: learning at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17916f2",
   "metadata": {},
   "source": [
    "<b>Temporal Difference, on the other hand, waits for only one interaction (one step) $S_{t+1}$</b> to form a TD target and update $V(S_{t})$ using $R_{t+1}$ and $\\gamma * V(S_{t+1})$.\n",
    "\n",
    "\n",
    "The idea with <b>TD is to update the $V(S_{t})$ at each step.</b>\n",
    "\n",
    "But because we didn’t experience an entire episode, we don’t have $G_{t}$ (expected return). Instead, <b>we estimate $G_{t}$ by adding $R_{t+1}$ and the discounted value of the next state.</b>\n",
    "\n",
    "This is called bootstrapping. It’s called this <b>because TD bases its update in part on an existing estimate $V(S_{t+1})$ and not a complete sample $G_{t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450ecee",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" style=\"width:700px;\" title=\"Temporal Difference\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af50417",
   "metadata": {},
   "source": [
    "<span style=\"font-size:large\">$$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})]$$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7654c10",
   "metadata": {},
   "source": [
    "This method is called TD(0) or <b>one-step TD (update the value function after any individual step).</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg\" style=\"width:700px;\" title=\"Temporal Difference\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6ac68",
   "metadata": {},
   "source": [
    "If we take the same example,\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2.jpg\" style=\"width:700px;\" title=\"Temporal Difference\">\n",
    "    \n",
    "- We initialize our value function so that it returns 0 value for each state.\n",
    "- Our learning rate (lr) is 0.1, and our discount rate is 1 (no discount).\n",
    "    - Our mouse begins to explore the environment and takes a random action: <b>going to the left</b>\n",
    "- It gets a reward $R_{t+1} = 1$ since it eats a piece of cheese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b7999",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2p.jpg\" style=\"width:700px;\" title=\"Temporal Difference\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3.jpg\" style=\"width:700px;\" title=\"Temporal Difference\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52bd3e6e",
   "metadata": {},
   "source": [
    "We can now update $V(S_{0})$:\n",
    "    \n",
    "New $V(S_{0}) = V(S_{0}) + lr * [R_{1} + \\gamma*V(S_{1}) - V(S_{0})]$\n",
    "\n",
    "New $V(S_{0}) = 0 + 0.1*[1 + 1*0-0]$ \n",
    "\n",
    "New $V(S_{0}) = 0.1$\n",
    "\n",
    "So we just updated our value function for State 0.\n",
    "\n",
    "Now we<b> continue to interact with this environment with our updated value function.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3p.jpg\" style=\"width:700px;\" title=\"Temporal Difference\">\n",
    "\n",
    "To summarize:\n",
    "- With <i>Monte Carlo</i>, we update the value function from a complete episode, and so we <b>use the actual accurate discounted return of this episode.</b>\n",
    "- With <i>TD Learning</i>, we update the value function from a step, and we replace $G{t}$ which we don’t know, with <b>an estimated return called the TD target.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Summary.jpg\" style=\"width:700px;\" title=\"Summary\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e7ff8",
   "metadata": {},
   "source": [
    "# Mid-way Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889579b4",
   "metadata": {},
   "source": [
    "Before diving into Q-Learning, let’s summarize what we’ve just learned.\n",
    "\n",
    "We have two types of value-based functions:\n",
    "\n",
    "- State-value function: outputs the expected return if <b>the agent starts at a given state and acts according to the policy forever after.</b>\n",
    "- Action-value function: outputs the expected return if <b>the agent starts in a given state, takes a given action at that state</b> and then acts accordingly to the policy forever after.\n",
    "- In value-based methods, rather than learning the policy, we <b>define the policy by hand</b> and we learn a value function. If we have an optimal value function, we <b>will have an optimal policy.</b>\n",
    "\n",
    "There are two types of methods to learn a policy for a value function:\n",
    "- With the Monte Carlo method, we update the value function from a complete episode, and so we <b>use the actual discounted return of this episode.</b>\n",
    "- With the TD Learning method, we update the value function from a step, replacing the unknown with <b>an estimated return called the TD target.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66908943",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/summary-learning-mtds.jpg\" style=\"width:700px;\" title=\"Summary: Monte-Carlo vs Temporal Difference Learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60508e2",
   "metadata": {},
   "source": [
    "# Mid-way Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa2407",
   "metadata": {},
   "source": [
    "[2.7-Mid-way-Quiz.pdf](./2.7-Mid-way-Quiz.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd709e9",
   "metadata": {},
   "source": [
    "# Introducing Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b8b65",
   "metadata": {},
   "source": [
    "## What is Q-Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef58a29",
   "metadata": {},
   "source": [
    "Q-Learning is an <span style=\"color:blue\"><b>off-policy value-based method that uses a TD approach to train its action-value function:</b></span>\n",
    "\n",
    "- <i>Off-policy</i>: we’ll talk about that at the end of this unit.\n",
    "- <i>Value-based method</i>: finds the optimal policy indirectly by training a value or action-value function that will tell us the <b>value of each state or each state-action pair.</b>\n",
    "- <i>TD approach</i>: <b>updates its action-value function at each step instead of at the end of the episode.</b>\n",
    "\n",
    "<span style=\"color:blue\"><b>Q-Learning is the algorithm we use to train our Q-function, an action-value function </b>that determines the value of being at a particular state and taking a specific action at that state.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ea3ee",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg\" style=\"width:700px;\" title=\"Q-function\">\n",
    "<span style=\"font-size:small\"><i>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b30fc6",
   "metadata": {},
   "source": [
    "The <b>Q comes from “the Quality” (the value) of that action at that state.</b>\n",
    "\n",
    "Let’s recap the difference between value and reward:\n",
    "- <span style=\"color:blue\">The <i>value of a state, or a state-action pair</i> is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to its policy.</span>\n",
    "- <span style=\"color:blue\">The <i>reward</i> is the <b>feedback I get from the environment</b> after performing an action at a state.</span>\n",
    "\n",
    "Internally, our Q-function is encoded by a <b>Q-table, a table where each cell corresponds to a state-action pair value</b>. Think of this Q-table as <b>the memory or cheat sheet of our Q-function.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e7b7c",
   "metadata": {},
   "source": [
    "Let’s go through an example of a maze.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-1.jpg\" style=\"width:300px;\" title=\"Maze example\">\n",
    "\n",
    "The Q-table is initialized. That’s why all values are = 0. This table <b>contains, for each state and action, the corresponding state-action values.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-2.jpg\" style=\"width:700px;\" title=\"Maze example\">\n",
    "Here we see that the <b>state-action value of the initial state and going up is 0:</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-3.jpg\" style=\"width:700px;\" title=\"Maze example\">\n",
    "So: the Q-function uses a Q-table <b>that has the value of each state-action pair</b>. Given a state and action, <b>our Q-function will search inside its Q-table to output the value.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b6101",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" style=\"width:700px;\" title=\"Q function\">\n",
    "\n",
    "If we recap, <i>Q-Learning</i> <b>is the RL algorithm that:</b>\n",
    "- Trains a <i>Q-function</i> (an <b>action-value function</b>), which internally is a <b>Q-table that contains all the state-action pair values.</b>\n",
    "- Given a state and action, our Q-function <b>will search its Q-table for the corresponding value.</b>\n",
    "- When the training is done, <b>we have an optimal Q-function, which means we have optimal Q-table.</b>\n",
    "- And if <b>we have an optimal Q-function</b>, we <b>have an optimal policy</b> since we <b>know the best action to take at each state.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428234a",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" style=\"width:700px;\" title=\"Link value policy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf973a",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">In the beginning, <b>our Q-table is useless since it gives arbitrary values for each state-action pair</b> (most of the time, we initialize the Q-table to 0).</span><span style=\"color:green\"> As the agent <b>explores the environment and we update the Q-table, it will give us a better and better approximation </b>to the optimal policy.</span>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-1.jpg\" style=\"width:800px;\" title=\"Q-learning\">\n",
    "<span style=\"font-size:small\"><i>We see here that with the training, our Q-table is better since, thanks to it, we can know the value of each state-action pair.</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d21107",
   "metadata": {},
   "source": [
    "Now that we understand what Q-Learning, Q-functions, and Q-tables are, <b>let’s dive deeper into the Q-Learning algorithm.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c17a99",
   "metadata": {},
   "source": [
    "## The Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6581603",
   "metadata": {},
   "source": [
    "This is the Q-Learning pseudocode; let’s study each part and <b>see how it works with a simple example before implementing it</b>. Don’t be intimidated by it, it’s simpler than it looks! We’ll go over each step.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" style=\"width:800px;\" title=\"Q-learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01a1d3",
   "metadata": {},
   "source": [
    "- GLIE: Greedy in the Limit with Infinite Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e27ef",
   "metadata": {},
   "source": [
    "### Step 1: We initialize the Q-table\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg\" style=\"width:700px;\" title=\"Q-learning\">\n",
    "\n",
    "We need to initialize the Q-table for each state-action pair. <b>Most of the time, we initialize with values of 0.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4cc86",
   "metadata": {},
   "source": [
    "### Step 2: Choose an action using the epsilon-greedy strategy\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" style=\"width:650px;\" title=\"Q-learning\">\n",
    "\n",
    "The epsilon-greedy strategy is a policy that handles the exploration/exploitation trade-off.\n",
    "\n",
    "The idea is that, with an initial value of ɛ = 1.0:\n",
    "- With <i>probability 1 — ɛ</i> : we do <b>exploitation</b> (aka our agent selects the action with the highest state-action pair value).\n",
    "- With <i>probability ɛ</i>: we do <b>exploration</b> (trying random action).\n",
    "\n",
    "At the beginning of the training, <b>the probability of doing exploration will be huge since ɛ is very high, so most of the time, we’ll explore</b>. But as the training goes on, and consequently our <b>Q-table gets better and better in its estimations, we progressively reduce the epsilon value</b> since we will need less and less exploration and more exploitation.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg\" style=\"width:350px;\" title=\"Q-learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae5127",
   "metadata": {},
   "source": [
    "### Step 3: Perform action At, get reward Rt+1 and next state St+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720cbb9",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-6.jpg\" style=\"width:700px;\" title=\"Q-learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e192d",
   "metadata": {},
   "source": [
    "### Step 4: Update Q(St, At)\n",
    "Remember that in TD Learning, we update our policy or value function (depending on the RL method we choose) <b>after one step of the interaction.</b>\n",
    "\n",
    "To produce our TD target, <b>we used the immediate reward $R_{t+1}$ plus the discounted value of the next state, </b> computed by finding the action that maximizes the current Q-function at the next state. (We call that bootstrap).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg\" style=\"width:700px;\" title=\"Q-learning\">\n",
    "\n",
    "Therefore, our $Q(S_{t}, A_{t})$ <b>update formula goes like this:</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg\" style=\"width:700px;\" title=\"Q-learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23e53b",
   "metadata": {},
   "source": [
    "This means that to update our $Q(S_{t}, A_{t})$:\n",
    "- We need $S_t$, $A_t$, $R_{t+1}$, $S_{t+1}$ .\n",
    "- To update our Q-value at a given state-action pair, we use the TD target.\n",
    "\n",
    "How do we form the TD target?\n",
    "1.  <span style=\"color:blue\">We obtain the reward after taking the action $R_{t+1}$.</span>\n",
    "2.  <span style=\"color:blue\">To get the <b>best state-action pair value</b> for the next state, we use a greedy policy to select the next best action. <i>Note that this is not an epsilon-greedy policy, this will always take the action with the highest state-action value.</i></span>\n",
    "\n",
    "<span style=\"color:blue\">Then when the update of this Q-value is done, we start in a new state and select our action <b>using a epsilon-greedy policy again.</b></span>\n",
    "\n",
    "<span style=\"color:blue\"><b>This is why we say that Q Learning is an off-policy algorithm.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88799d9",
   "metadata": {},
   "source": [
    "## Off-policy vs On-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ea3ac",
   "metadata": {},
   "source": [
    "The difference is subtle:\n",
    "\n",
    "- <i>Off-policy</i>: using <b>a different policy for acting (inference) and updating (training).</b>\n",
    "\n",
    "For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the greedy policy that <b>is used to select the best next-state action value to update our Q-value (updating policy).</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce13d7",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-1.jpg\" style=\"width:600px;\" title=\"Off-on policy\">\n",
    "<span style=\"font-size:small;\"><b><i>Action policy</i></b></span>\n",
    "\n",
    "Is different from the policy we use during the training part:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-2.jpg\" style=\"width:200px;\" title=\"Acting Policy\">\n",
    "<span style=\"font-size:small\"><b><i>Updating policy</i></b></span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30ad96c6",
   "metadata": {},
   "source": [
    "- <i>On-policy</i>: using the <b>same policy for acting and updating.</b>\n",
    "\n",
    "For instance, with Sarsa, another value-based algorithm, <b>the epsilon-greedy policy selects the next state-action pair, not a greedy policy.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-3.jpg\" style=\"width:600px;\" title=\"Off-on policy\">\n",
    "<span style=\"font-size:small;\"><i>Sarsa</i></span>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" style=\"width:600px;\" title=\"Off-on policy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2810b",
   "metadata": {},
   "source": [
    "# A Q-Learning example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f0535",
   "metadata": {},
   "source": [
    "To better understand Q-Learning, let’s take a simple example:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-Example-2.jpg\" style=\"width:600px;\" title=\"Maze-Example\">\n",
    "\n",
    "- You’re a mouse in this tiny maze. You always <b>start at the same starting point.</b>\n",
    "- The goal is <b>to eat the big pile of cheese at the bottom right-hand corner</b> and avoid the poison. After all, who doesn’t like cheese?\n",
    "- The episode ends if we eat the poison, <b>eat the big pile of cheese</b>, or if we take more than five steps.\n",
    "- The learning rate is 0.1\n",
    "- The discount rate (gamma) is 0.99\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-1.jpg\" style=\"width:600px;\" title=\"Maze-Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721afc4f",
   "metadata": {},
   "source": [
    "The reward function goes like this:\n",
    "\n",
    "- <b>+0</b>: Going to a state with no cheese in it.\n",
    "- <b>+1</b>: Going to a state with a small cheese in it.\n",
    "- <b>+10</b>: Going to the state with the big pile of cheese.\n",
    "- <b>-10</b>: Going to the state with the poison and thus dying.\n",
    "- <b>+0</b> If we take more than five steps.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-2.jpg\" style=\"width:600px;\" title=\"Maze-Example\">\n",
    "\n",
    "To train our agent to have an optimal policy (so a policy that goes right, right, down), <b>we will use the Q-Learning algorithm.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f5580",
   "metadata": {},
   "source": [
    "## Step 1: Initialize the Q-table\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-1.jpg\" style=\"width:600px;\" title=\"Maze-Example\">\n",
    "So, for now, <b>our Q-table is useless</b>; we need <b>to train our Q-function using the Q-Learning algorithm.</b>\n",
    "\n",
    "Let’s do it for 2 training timesteps:\n",
    "\n",
    "<span style=\"color:blue\">Training timestep 1:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ed36d",
   "metadata": {},
   "source": [
    "## Step 2: Choose an action using the Epsilon Greedy Strategy\n",
    "Because epsilon is big (= 1.0), I take a random action. In this case, I go right.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-3.jpg\" style=\"width:600px;\" title=\"Maze-Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee277af",
   "metadata": {},
   "source": [
    "## Step 3: Perform action At, get Rt+1 and St+1\n",
    "By going right, I get a small cheese, so $R_{t+1}=1$ and I’m in a new state.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-4.jpg\" style=\"width:600px;\" title=\"Maze-Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2cddf",
   "metadata": {},
   "source": [
    "## Step 4: Update Q(St, At)\n",
    "We can now update $Q(S_{t}, A_{t})$ using our formula.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg\" style=\"width:600px;\" title=\"Maze-Example\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-4.jpg\" style=\"width:600px;\" title=\"Maze-Example\">\n",
    "\n",
    "Training timestep 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d15993",
   "metadata": {},
   "source": [
    "## Step 2: Choose an action using the Epsilon Greedy Strategy\n",
    "<b>I take a random action again, since epsilon=0.99 is big</b>. (Notice we decay epsilon a little bit because, as the training progress, we want less and less exploration).\n",
    "\n",
    "I took the action ‘down’. <b>This is not a good action since it leads me to the poison.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-6.jpg\" style=\"width:600px;\" title=\"Maze-Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e473088",
   "metadata": {},
   "source": [
    "## Step 3: Perform action At, get Rt+1 and St+1\n",
    "Because I ate poison, <b>I get $R_{t+1} = -10$, and I die.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-7.jpg\" style=\"width:600px;\" title=\"Maze-Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c8c2a",
   "metadata": {},
   "source": [
    "## Step 4: Update Q(St, At)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-8.jpg\" style=\"width:600px;\" title=\"Maze-Example\">\n",
    "\n",
    "Because we’re dead, we start a new episode. But what we see here is that, <b>with two explorations steps, my agent became smarter.</b>\n",
    "\n",
    "As we continue exploring and exploiting the environment and updating Q-values using the TD target, <b>the Q-table will give us a better and better approximation. At the end of the training, we’ll get an estimate of the optimal Q-function.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645fdf9",
   "metadata": {},
   "source": [
    "# Q-Learning Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95e1a0",
   "metadata": {},
   "source": [
    "<i>Q-Learning</i> <b>is the RL algorithm that :</b>\n",
    "\n",
    "- Trains a<i> Q-function</i>, an <b>action-value function</b> encoded, in internal memory, by a Q-table <b>containing all the state-action pair values.</b>\n",
    "- Given a state and action, our Q-function <b>will search its Q-table for the corresponding value.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" style=\"width:600px;\" title=\"Q function\">\n",
    "\n",
    "- When the training is done, <b>we have an optimal Q-function, or, equivalently, an optimal Q-table.</b>\n",
    "- And if we <b>have an optimal Q-function</b>, we have an optimal policy, since we <b>know, for each state, the best action to take.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a12b6",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" style=\"width:600px;\" title=\"Link value policy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06763f",
   "metadata": {},
   "source": [
    "But, in the beginning, our <b>Q-table is useless since it gives arbitrary values for each state-action pair (most of the time we initialize the Q-table to 0 values)</b>. But, as we explore the environment and update our Q-table it will give us a better and better approximation.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" style=\"width:700px;\" title=\"Q learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0aa0ae",
   "metadata": {},
   "source": [
    "This is the Q-Learning pseudocode:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" style=\"width:700px;\" title=\"Q learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af935290",
   "metadata": {},
   "source": [
    "# Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f8955",
   "metadata": {},
   "source": [
    "## Strategies to find the optimal policy\n",
    "- <b>Policy-based methods</b>. The policy is usually trained with a neural network to select what action to take given a state. In this case it is the neural network which outputs the action that the agent should take instead of using a value function. Depending on the experience received by the environment, the neural network will be re-adjusted and will provide better actions.\n",
    "- <b>Value-based methods</b>. In this case, a value function is trained to output the value of a state or a state-action pair that will represent our policy. However, this value doesn’t define what action the agent should take. In contrast, we need to specify the behavior of the agent given the output of the value function. For example, we could decide to adopt a policy to take the action that always leads to the biggest reward (Greedy Policy). In summary, the policy is a Greedy Policy (or whatever decision the user takes) that uses the values of the value-function to decide the actions to take.\n",
    "\n",
    "<b>Among the value-based methods, we can find two main strategies</b>\n",
    "- <b>The state-value function</b>. For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.\n",
    "- <b>The action-value function</b>. In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state and takes an action. Then it follows the policy forever after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6576f2",
   "metadata": {},
   "source": [
    "## Epsilon-greedy strategy:\n",
    "\n",
    "- Common strategy used in reinforcement learning that involves balancing exploration and exploitation.\n",
    "- Chooses the action with the highest expected reward with a probability of 1-epsilon.\n",
    "- Chooses a random action with a probability of epsilon.\n",
    "- Epsilon is typically decreased over time to shift focus towards exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27cffa",
   "metadata": {},
   "source": [
    "## Greedy strategy:\n",
    "- Involves always choosing the action that is expected to lead to the highest reward, based on the current knowledge of the environment. (Only exploitation)\n",
    "- Always chooses the action with the highest expected reward.\n",
    "- Does not include any exploration.\n",
    "- Can be disadvantageous in environments with uncertainty or unknown optimal actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc15f7",
   "metadata": {},
   "source": [
    "## Off-policy vs on-policy algorithms\n",
    "- <b>Off-policy algorithms</b>: A different policy is used at training time and inference time\n",
    "- <b>On-policy algorithms</b>: The same policy is used during training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6299c2",
   "metadata": {},
   "source": [
    "## Monte Carlo and Temporal Difference learning strategies\n",
    "- <b>Monte Carlo (MC)</b>: Learning at the end of the episode. With Monte Carlo, we wait until the episode ends and then we update the value function (or policy function) from a complete episode.\n",
    "- <b>Temporal Difference (TD)</b>: Learning at each step. With Temporal Difference Learning, we update the value function (or policy function) at each step without requiring a complete episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ee60c",
   "metadata": {},
   "source": [
    "# Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b8d0b",
   "metadata": {},
   "source": [
    "## q-Frozenlake-v1-4x4-noSlippery\n",
    "\n",
    "- My [model card](https://huggingface.co/prasanthntu/q-FrozenLake-v1-4x4-noSlippery) in huggingface.\n",
    "    - It can be found under the `Reinforcement learning` [model libraries](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=trending&search=prasanthntu) section.\n",
    "- My [source code](https://colab.research.google.com/drive/1SSmGmegUEqArlttm9nM1omkkBefxt-VU?usp=sharing) in Google Colab stored in Google Drive for building the model.\n",
    "\n",
    "<b>Summary</b>\n",
    "- <b>Goal</b>:  Train our agent to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H).\n",
    "- <b>Environment</b>\n",
    "    - [Gymnasium](https://gymnasium.farama.org/environments/toy_text/frozen_lake/#frozen-lake) - To create RL environments  \n",
    "        - FrozenLake-v1 - For 4x4 env\n",
    "            - Observation space: 1D vector\n",
    "                - `Discrete(16)`\n",
    "            - Action space: Scalar\n",
    "                - `Discrete(4)` $\\Rightarrow$ No. of actions available: 4\n",
    "            - Rewards\n",
    "                - Reach goal: +1\n",
    "                - Reach hole: 0\n",
    "                - Reach frozen: 0\n",
    "            - Episode end\n",
    "                - Termination\n",
    "                    - The player moves into a hole.\n",
    "                    - The player reaches the goal.                 \n",
    "                - Truncation\n",
    "                    - When exceeding the length of the interactions in a episode \n",
    "                        - 100 timesteps for 4x4 env.\n",
    "- <b>Model</b>\n",
    "    - Q learning - Built from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ddcef9",
   "metadata": {},
   "source": [
    "## q-Taxi-v3\n",
    "\n",
    "- My [model card](https://huggingface.co/prasanthntu/q-Taxi-v3) in huggingface.\n",
    "    - It can be found under the `Reinforcement learning` [model libraries](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=trending&search=prasanthntu) section.\n",
    "- My [source code](https://colab.research.google.com/drive/1SSmGmegUEqArlttm9nM1omkkBefxt-VU?usp=sharing) in Google Colab stored in Google Drive for building the model.\n",
    "\n",
    "<b>Summary</b>\n",
    "- <b>Goal</b>:  Train our agent to navigate to passengers in a grid world, picking them up and dropping them off at one of four designated locations.\n",
    "- <b>Environment</b>\n",
    "    - [Gymnasium](https://gymnasium.farama.org/environments/toy_text/taxi/) - To create RL environments  \n",
    "        - Taxi-v3 \n",
    "            - Observation space: 1D vector\n",
    "                - `Discrete(500)` as there are \n",
    "                    - (5x5 grid) 25 taxi positions\n",
    "                    - 5 passenger locations (technically, 4 passenger locations + case when passenger is in the taxi)\n",
    "                    - 4 destination locations\n",
    "            - Action space: Scalar\n",
    "                - `Discrete(6)` $\\Rightarrow$ No. of actions available: 6\n",
    "                - Possible actions\n",
    "                    - 0: Move south (down)\n",
    "                    - 1: Move north (up)\n",
    "                    - 2: Move east (right)\n",
    "                    - 3: Move west (left)\n",
    "                    - 4: Pickup passenger\n",
    "                    - 5: Drop off passenger\n",
    "            - Rewards\n",
    "                - -1 per step unless other reward is triggered.\n",
    "                - +20 delivering passenger.\n",
    "                - -10 executing “pickup” and “drop-off” actions illegally.\n",
    "                \n",
    "              An action that results a noop, like moving into a wall, will incur the time step penalty. Noops can be avoided by sampling the action_mask returned in info.\n",
    "              \n",
    "            - Episode end\n",
    "                - Termination\n",
    "                    - The taxi drops off the passenger.                 \n",
    "                - Truncation\n",
    "                    - When exceeding the length of the interactions in a episode \n",
    "                        - 200 timesteps.\n",
    "- <b>Model</b>\n",
    "    - Q learning - Built from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89b904",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48561010",
   "metadata": {},
   "source": [
    "# Q-Learning Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8de731",
   "metadata": {},
   "source": [
    "[2.13-Quiz](./2.13-Quiz.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc01fe",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed4038",
   "metadata": {},
   "source": [
    "Congrats on finishing this chapter! There was a lot of information. And congrats on finishing the tutorials. You’ve just implemented your first RL agent from scratch and shared it on the Hub 🥳.\n",
    "\n",
    "Implementing from scratch when you study a new architecture <b>is important to understand how it works.</b>\n",
    "\n",
    "It’s normal if you still feel confused by all these elements. <b>This was the same for me and for everyone who studies RL.</b>\n",
    "\n",
    "Take time to really grasp the material before continuing.\n",
    "\n",
    "In the next chapter, we’re going to dive deeper by studying our first Deep Reinforcement Learning algorithm based on Q-Learning: Deep Q-Learning. And you’ll train a <b>DQN agent with [RL-Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) to play Atari Games.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffec714",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" style=\"width:600px;\" title=\"Atari environments\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6c950",
   "metadata": {},
   "source": [
    "# Additional Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df4f89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-20T03:37:24.162775Z",
     "start_time": "2023-08-20T03:37:24.149530Z"
    }
   },
   "source": [
    "These are <b>optional readings</b> if you want to go deeper.\n",
    "\n",
    "## Monte Carlo and TD Learning\n",
    "To dive deeper into Monte Carlo and Temporal Difference Learning:\n",
    "- [Why do temporal difference (TD) methods have lower variance than Monte Carlo methods?](https://stats.stackexchange.com/questions/355820/why-do-temporal-difference-td-methods-have-lower-variance-than-monte-carlo-met)\n",
    "- [When are Monte Carlo methods preferred over temporal difference ones?](https://stats.stackexchange.com/questions/336974/when-are-monte-carlo-methods-preferred-over-temporal-difference-ones)\n",
    "\n",
    "## Q-Learning\n",
    "- [Reinforcement Learning: An Introduction, Richard Sutton and Andrew G. Barto Chapter 5, 6 and 7](http://incompleteideas.net/book/RLbook2020.pdf)\n",
    "- [Foundations of Deep RL Series, L2 Deep Q-Learning by Pieter Abbeel](https://youtu.be/Psrhxy88zww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e2345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "790px",
    "left": "52px",
    "top": "111.125px",
    "width": "471px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
