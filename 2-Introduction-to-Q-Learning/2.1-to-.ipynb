{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a06359b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#What-is-RL?-A-short-recap\" data-toc-modified-id=\"What-is-RL?-A-short-recap-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What is RL? A short recap</a></span></li><li><span><a href=\"#The-two-types-of-value-based-methods\" data-toc-modified-id=\"The-two-types-of-value-based-methods-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The two types of value-based methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-state-value-function\" data-toc-modified-id=\"The-state-value-function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>The state-value function</a></span></li><li><span><a href=\"#The-action-value-function\" data-toc-modified-id=\"The-action-value-function-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The action-value function</a></span></li></ul></li><li><span><a href=\"#The-Bellman-Equation,-simplify-our-value-estimation\" data-toc-modified-id=\"The-Bellman-Equation,-simplify-our-value-estimation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>The Bellman Equation, simplify our value estimation</a></span></li><li><span><a href=\"#Monte-Carlo-vs-Temporal-Difference-Learning\" data-toc-modified-id=\"Monte-Carlo-vs-Temporal-Difference-Learning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Monte Carlo vs Temporal Difference Learning</a></span></li><li><span><a href=\"#Mid-way-Recap\" data-toc-modified-id=\"Mid-way-Recap-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Mid-way Recap</a></span></li><li><span><a href=\"#Mid-way-Quiz\" data-toc-modified-id=\"Mid-way-Quiz-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Mid-way Quiz</a></span></li><li><span><a href=\"#Introducing-Q-Learning\" data-toc-modified-id=\"Introducing-Q-Learning-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Introducing Q-Learning</a></span></li><li><span><a href=\"#A-Q-Learning-example\" data-toc-modified-id=\"A-Q-Learning-example-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>A Q-Learning example</a></span></li><li><span><a href=\"#Q-Learning-Recap\" data-toc-modified-id=\"Q-Learning-Recap-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Q-Learning Recap</a></span></li><li><span><a href=\"#Glossary\" data-toc-modified-id=\"Glossary-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Glossary</a></span></li><li><span><a href=\"#Hands-on\" data-toc-modified-id=\"Hands-on-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Hands-on</a></span></li><li><span><a href=\"#Q-Learning-Quiz\" data-toc-modified-id=\"Q-Learning-Quiz-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Q-Learning Quiz</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Additional-Readings\" data-toc-modified-id=\"Additional-Readings-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Additional Readings</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d20de",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe2d837",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/deep-rl-course/unit2/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87091e7b",
   "metadata": {},
   "source": [
    "In the first unit of this class, we learned about Reinforcement Learning (RL), the RL process, and the different methods to solve an RL problem. We also <b>trained our first agents and uploaded them to the Hugging Face Hub</b>.\n",
    "\n",
    "In this unit, we’re going to <b>dive deeper into one of the Reinforcement Learning methods: value-based methods</b> and study our <b>first RL algorithm: Q-Learning.</b>\n",
    "\n",
    "We’ll also <b>implement our first RL agent from scratch</b>, a Q-Learning agent, and will train it in two environments:\n",
    "- Frozen-Lake-v1 (non-slippery version): where our agent will need to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoiding holes (H).\n",
    "- An autonomous taxi: where our agent will need to learn to navigate a city to transport its passengers from point A to point B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d835e",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" style=\"width:700px;\" title=\"Two environments\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b87ad",
   "metadata": {},
   "source": [
    "Concretely, we will:\n",
    "\n",
    "- Learn about <b>value-based methods</b>.\n",
    "- Learn about the <b>differences between Monte Carlo and Temporal Difference Learning</b>.\n",
    "- Study and implement <b>our first RL algorithm</b>: Q-Learning.\n",
    "\n",
    "This unit is <b>fundamental if you want to be able to work on Deep Q-Learning</b>: the first Deep RL algorithm that played Atari games and beat the human level on some of them (breakout, space invaders, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8da45",
   "metadata": {},
   "source": [
    "# What is RL? A short recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e9367",
   "metadata": {},
   "source": [
    "In RL, we build an agent that can <b>make smart decisions</b>. For instance, an agent that <b>learns to play a video game</b>. Or a trading agent that <b>learns to maximize its benefits</b> by deciding on <b>what stocks to buy and when to sell</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa633ef2",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/rl-process.jpg\" style=\"width:700px;\" title=\"RL Process\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c184c",
   "metadata": {},
   "source": [
    "To make intelligent decisions, our agent will learn from the environment by <b>interacting with it through trial and error and</b> receiving rewards (positive or negative) as <b>unique feedback</b>.\n",
    "\n",
    "Its goal is to <b>maximize its expected cumulative reward</b> (because of the reward hypothesis).\n",
    "\n",
    "The <b>agent’s decision-making process is called the policy π</b>: given a state, a policy will output an action or a probability distribution over actions. That is, given an observation of the environment, a policy will provide an action (or multiple probabilities for each action) that the agent should take.\n",
    "\n",
    "<img src=\"\n",
    "https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/policy.jpg\" style=\"width:700px;\" title=\"Policy: The agent's brain\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e038f5",
   "metadata": {},
   "source": [
    "Our <b>goal is to find an optimal policy π*</b>, aka., a policy that leads to the best expected cumulative reward.\n",
    "\n",
    "And to find this optimal policy (hence solving the RL problem), there are <b>two main types of RL methods</b>:\n",
    "\n",
    "- <i>Policy-based methods</i>: <b>Train the policy directly</b> to learn which action to take given a state.\n",
    "- <i>Value-based methods</i>: <b>Train a value function</b> to learn <b>which state is more valuable</b> and use this value function to <b>take the action that leads to it</b>.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg\" style=\"width:700px;\" title=\"RL Process\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38896f",
   "metadata": {},
   "source": [
    "# The two types of value-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6facb",
   "metadata": {},
   "source": [
    "In value-based methods, <b>we learn a value function that maps a state to the expected value of being at that state.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b72194",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" style=\"width:700px;\" title=\"Value-based Methods\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c541655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T08:26:01.860464Z",
     "start_time": "2023-08-17T08:26:01.847788Z"
    }
   },
   "source": [
    "$$v_{\\pi}(s) = E_{\\pi}\\left [R_{t+1} + \\gamma R_{t+2} + \\gamma R_{t+3} + \\cdots | S_{t} = s \\right ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43623c",
   "metadata": {},
   "source": [
    "The value of a state is the <b>expected discounted return</b> the agent can get if it <b>starts at that state and then acts according to our policy.</b>\n",
    "\n",
    "> But what does it mean to act according to our policy? After all, we don't have a policy in value-based methods since we train a value function and not a policy.\n",
    "\n",
    "Remember that the goal of an <b>RL agent is to have an optimal policy π*.</b>\n",
    "\n",
    "To find the optimal policy, we learned about two different methods:\n",
    "\n",
    "- <i>Policy-based methods</i>: <b>Directly train the policy</b> to select what action to take given a state (or a probability distribution over actions at that state). In this case, we <b>don’t have a value function.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789acc6",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-2.jpg\" style=\"width:700px;\" title=\"Two approaches to find optimal policy: policy based methods\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a63eb",
   "metadata": {},
   "source": [
    "- <i>Policy-based methods</i>: The policy takes a state as input and outputs what action to take at that state (deterministic policy: a policy that output one action given a state, contrary to stochastic policy that output a probability distribution over actions).\n",
    "\n",
    "And consequently, <b>we don’t define by hand the behavior of our policy; it’s the training that will define it.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa132bf",
   "metadata": {},
   "source": [
    "- <i>Value-based methods</i>: <b>Indirectly, by training a value function</b> that outputs the value of a state or a state-action pair. Given this value function, our policy <b>will take an action.<b/>\n",
    "\n",
    "Since the policy is not trained/learned, <b>we need to specify its behavior</b>. For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward, <span style=\"color:blue\"><b>we’ll create a Greedy Policy.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e445345",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-3.jpg\" style=\"width:700px;\" title=\"Two approaches to find optimal policy: value based methods\">\n",
    "<span style=\"font-size:small; color:blue\"><i>Given a state, our action-value function (that we train) outputs the value of each action at that state. Then, our pre-defined Greedy Policy selects the action that will yield the highest value given a state or a state action pair.</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4b3fe",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Consequently, whatever method you use to solve your problem, <b>you will have a policy</b></span>. In the case of value-based methods, you don’t train the policy: your policy <b>is just a simple pre-specified function</b> (for instance, the Greedy Policy) that uses the values given by the value-function to select its actions.\n",
    "\n",
    "<b>So the difference is:</b>\n",
    "- <span style=\"color:blue\">In policy-based training, <b>the optimal policy (denoted π*) is found by training the policy directly.</b></span>\n",
    "- <span style=\"color:blue\">In value-based training, <b>finding an optimal value function (denoted Q* or V*, we’ll study the difference below) leads to having an optimal policy.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a4035",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" style=\"width:500px;\" title=\"The link between value and policy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39fd4b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:large\">$$\\pi^{*}(s) = arg \\, \\underset{a}{max} \\, Q^{*}(s,a)$$ </span>\n",
    "where\n",
    "- $\\pi^{*}$ is the optimal policy.\n",
    "- $Q^{*}$ is the optimal value function.\n",
    "- $\\underset{a}{max}$ is the pre-defined Greedy Policy that selects the action that will yield the highest value given a state or state action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff1fe6",
   "metadata": {},
   "source": [
    "In fact, most of the time, in value-based methods, you’ll use <span style=\"color:blue\"><b>an Epsilon-Greedy Policy</b></span> that handles the exploration/exploitation trade-off; we’ll talk about this when we talk about Q-Learning in the second part of this unit.\n",
    "\n",
    "As we mentioned above, we have two types of value-based functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b14bf9",
   "metadata": {},
   "source": [
    "## The state-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc77d84",
   "metadata": {},
   "source": [
    "We write the state value function under a policy π like this:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-1.jpg\" style=\"width:700px;\" title=\"State value function\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeea215",
   "metadata": {},
   "source": [
    "For each state, the state-value function outputs the expected return if the agent <b>starts at that state</b> and then follows the policy forever afterward (for all future timesteps, if you prefer).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-2.jpg\" style=\"width:700px;\" title=\"State value function\">\n",
    "<span style=\"font-size:small\"><i>If we take the state with value -7: it's the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right.</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ede6f9",
   "metadata": {},
   "source": [
    "## The action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88f860",
   "metadata": {},
   "source": [
    "n the action-value function, for each state and action pair, the action-value function <b>outputs the expected return</b> if the agent starts in that state, takes that action, and then follows the policy forever after.\n",
    "\n",
    "The value of taking action $a$ in state $s$  under a policy $/pi$ is:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-1.jpg\" style=\"width:700px;\" title=\"Action State value function\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-2.jpg\" style=\"width:700px;\" title=\"Action State value function\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346f5f1",
   "metadata": {},
   "source": [
    "We see that the difference is:\n",
    "- For the state-value function, we calculate <b>the value of a state $S_{t}$</b>\n",
    "- For the action-value function, we calculate <b>the value of the state-action pair ($S_{t}$, $A_{t}$) hence the value of taking that action at that state.</b>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-types.jpg\" style=\"width:700px;\" title=\"Two types of value function\">\n",
    "<span style=\"font-size:small\"><i>Note: We didn't fill all the state-action pairs for the example of Action-value function</i></span>\n",
    "\n",
    "—In either case, whichever value function we choose (state-value or action-value function), <b>the returned value is the expected return.</b>\n",
    "\n",
    "However, the problem is that <b>to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.</b>\n",
    "\n",
    "This can be a computationally expensive process, and that’s <b>where the Bellman equation comes in to help us.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a827283",
   "metadata": {},
   "source": [
    "# The Bellman Equation, simplify our value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f1209",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4c5b23c",
   "metadata": {},
   "source": [
    "# Monte Carlo vs Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd3e6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba4e7ff8",
   "metadata": {},
   "source": [
    "# Mid-way Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889579b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e60508e2",
   "metadata": {},
   "source": [
    "# Mid-way Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa2407",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd709e9",
   "metadata": {},
   "source": [
    "# Introducing Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad96c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35a2810b",
   "metadata": {},
   "source": [
    "# A Q-Learning example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c8c2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3645fdf9",
   "metadata": {},
   "source": [
    "# Q-Learning Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0aa0ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af935290",
   "metadata": {},
   "source": [
    "# Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6299c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f9ee60c",
   "metadata": {},
   "source": [
    "# Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89b904",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48561010",
   "metadata": {},
   "source": [
    "# Q-Learning Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8de731",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38cc01fe",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffec714",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48a6c950",
   "metadata": {},
   "source": [
    "# Additional Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1a2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "790px",
    "left": "52px",
    "top": "111.125px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
