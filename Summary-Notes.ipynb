{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f3a03d",
   "metadata": {},
   "source": [
    "# Unit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bafd20",
   "metadata": {},
   "source": [
    "<b>RL Process</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123c591",
   "metadata": {},
   "source": [
    "A loop of state, action, reward, and next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52ce883",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" alt=\"RL process\" style=\"width: 40%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" alt=\"RL loops output sequence\" style=\"width: 40%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba1ff6",
   "metadata": {},
   "source": [
    "<b>Markow's Decision Process</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744df25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T04:58:30.580786Z",
     "start_time": "2023-09-05T04:58:30.570331Z"
    }
   },
   "source": [
    "<b>State/Observation space:</b>\n",
    "- State space\n",
    "- Observation space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691b96b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T04:58:56.378653Z",
     "start_time": "2023-09-05T04:58:56.371034Z"
    }
   },
   "source": [
    "<b>Action space</b>\n",
    "- Continuous space\n",
    "- Discrete space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebac7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:03:41.661887Z",
     "start_time": "2023-09-05T05:03:41.642394Z"
    }
   },
   "source": [
    "<b>Reward</b>\n",
    "- Cumulative Reward, $R(\\tau)$\n",
    "- Discounted Expected Cumulative Reward\n",
    "    - Smaller discount => Larger gamma => Agent cares more about long-term reward (and vice versa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ffdb7",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg\" style=\"width:400px;\" title=\"Cumulative reward\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg\" style=\"width:150px;\" title=\"Cumulative reward\">\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d5bfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:04:37.120278Z",
     "start_time": "2023-09-05T05:04:37.109293Z"
    }
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\" style=\"width:400px;\" title=\"Discounted exepcted cumulative reward\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c1ff8",
   "metadata": {},
   "source": [
    "<b>Tasks</b>\n",
    "- Episodic tasks\n",
    "- Continuous tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24fee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:10:59.035176Z",
     "start_time": "2023-09-05T05:10:59.018090Z"
    }
   },
   "source": [
    "<b>Exploration-exploitation tradeoff</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b73cc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T03:01:33.492044Z",
     "start_time": "2023-09-06T03:01:33.462010Z"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" style=\" width:500px; padding: 10px 20px;\" title=\"Value-based Methods\">\n",
    "\n",
    "<b>Policy $\\pi$</b> - Brain of our agent\n",
    "\n",
    "<b>Two main approaches (Policy) for solving RL problems</b>\n",
    "\n",
    "- <b>Policy-based methods</b>: Learns a policy function.\n",
    "    - Two types:\n",
    "        - Deterministic: $a=\\pi (s)$\n",
    "        - Stochastic: $\\pi[a|s]=P[A|s]$\n",
    "    \n",
    "- <b>Value-based methods</b>: Learns a value function\n",
    "    - $v_{\\pi}(s)$ = $E_{\\pi}(R_{t+1} + R_{t+2} + ... |\\, S_{t}=s)$\n",
    "\n",
    "    - Two types (algos):\n",
    "        - <b>Q learning (traditional RL)</b>\n",
    "        - <b>Deep Q learning (Uses NN)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174de27",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" style=\"width:600px;\" title=\"Q learning vs. Deep Q learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99bed2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b150dd3",
   "metadata": {},
   "source": [
    "# Unit 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16792ab",
   "metadata": {},
   "source": [
    "<b>Value based methods</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa27c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:37:39.559924Z",
     "start_time": "2023-09-05T10:37:39.550095Z"
    }
   },
   "source": [
    "- <b>Greedy Policy</b>: Since policy is not trained/learned in the <b>value-based methods</b>, we define the specific behavior of the policy by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5f3fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:33:03.923032Z",
     "start_time": "2023-09-05T10:33:03.912372Z"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" style=\" width:500px; padding: 10px 20px;\" title=\"The link between value and policy\">\n",
    "\n",
    "> <i>Link between value and policy: </i>\n",
    "> $\\pi^{*}(s) = arg \\, \\underset{a}{max} \\, Q^{*}(s,a)$, where\n",
    "> - $\\pi^{*}$ is the optimal policy.\n",
    "> - $Q^{*}$ is the optimal value function.\n",
    "> - $\\underset{a}{max}$ is the pre-defined greedy policy that selects the action that yields the highest expected cumulative value given the state or state action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8216e15",
   "metadata": {},
   "source": [
    "- <b>Epsilon-Greedy Policy</b> Policy that handles exploration/exploitation tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348dfb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:38:41.055470Z",
     "start_time": "2023-09-05T10:38:41.044070Z"
    }
   },
   "source": [
    "<b>Two main types (strategies) of value based functions:</b>\n",
    "- <b>state-value function, $V$</b>\n",
    "    - $V_{\\pi}(s) = E_{\\pi}[G_{t}|S_{t}=s]$\n",
    "- <b>action-value function, $Q$</b>\n",
    "    - $Q_{\\pi}(s,a) = E_{\\pi}[G_{t}|S_{t}=s, A_{t}=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb3803",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg\" style=\" width:500px; padding: 10px 20px;\" title=\"Bellman equation\">\n",
    "\n",
    "<span style=\"color:red\"><i>Computationally expenseive Problem</i>: To claculate each value of a state $V_{\\pi}(s)$ or state-action pair $Q_{\\pi}(s,a)$, we need to sum all the rewards an agent can get if it starts at that state, and followed the policy forever afterwards.</span>\n",
    "\n",
    "<b>Bellman's equation:</b> Simplifies our value estimation.\n",
    "\n",
    "$V_{\\pi}(s) = E[R_{t+1} + \\gamma * V_{\\pi}(S_{t+1})| S_{t}=s)]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d011069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "814px",
    "left": "30px",
    "top": "143px",
    "width": "345.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
