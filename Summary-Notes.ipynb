{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b8e6d65",
   "metadata": {},
   "source": [
    "# Unit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128cd29",
   "metadata": {},
   "source": [
    "Unit 1 Key concepts:<br> \n",
    "<i>RL Process $\\rightarrow$ MDP $\\rightarrow$ State/Observation Space $\\rightarrow$ Action space $\\rightarrow$ Rewards $\\rightarrow$ Tasks $\\rightarrow$ Exploration/exploitation tradeoff $\\rightarrow$ Policy $\\rightarrow$ Two main approches of solving RL problems $\\rightarrow$ Policy based (Deterministic vs. Stochastic) $\\rightarrow$ Value based $\\rightarrow$ Q learning vs. Deep Q-learning </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2573b",
   "metadata": {},
   "source": [
    "<b>RL Process</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea3359",
   "metadata": {},
   "source": [
    "A loop of state, action, reward, and next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f692f9a",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" alt=\"RL process\" style=\"width: 50%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" alt=\"RL loops output sequence\" style=\"width: 30%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0cf8ff",
   "metadata": {},
   "source": [
    "<b>Markow's Decision Process</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435efbc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T04:58:30.580786Z",
     "start_time": "2023-09-05T04:58:30.570331Z"
    }
   },
   "source": [
    "<b>State/Observation space:</b>\n",
    "- State space\n",
    "- Observation space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5954fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T04:58:56.378653Z",
     "start_time": "2023-09-05T04:58:56.371034Z"
    }
   },
   "source": [
    "<b>Action space</b>\n",
    "- Continuous space\n",
    "- Discrete space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030cfb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:03:41.661887Z",
     "start_time": "2023-09-05T05:03:41.642394Z"
    }
   },
   "source": [
    "<b>Reward</b>\n",
    "- Cumulative Reward, $R(\\tau)$\n",
    "- Discounted Expected Cumulative Reward\n",
    "    - Smaller discount => Larger gamma => Agent cares more about long-term reward (and vice versa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2deda36",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg\" style=\"width:400px;\" title=\"Cumulative reward\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg\" style=\"width:150px;\" title=\"Cumulative reward\">\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86727fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:04:37.120278Z",
     "start_time": "2023-09-05T05:04:37.109293Z"
    }
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\" style=\"width:400px;\" title=\"Discounted exepcted cumulative reward\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f738925",
   "metadata": {},
   "source": [
    "<b>Tasks</b>\n",
    "- Episodic tasks\n",
    "- Continuous tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e57af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:10:59.035176Z",
     "start_time": "2023-09-05T05:10:59.018090Z"
    }
   },
   "source": [
    "<b>Exploration-exploitation tradeoff</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00487357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T03:01:33.492044Z",
     "start_time": "2023-09-06T03:01:33.462010Z"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" style=\" width:500px; padding: 10px 20px;\" title=\"Value-based Methods\">\n",
    "\n",
    "<b>Policy $\\pi$</b> - Brain of our agent\n",
    "\n",
    "<b>Two main approaches (Policy) for solving RL problems</b>\n",
    "\n",
    "- <b>Policy-based methods</b>: Learns a policy function.\n",
    "    - Two types:\n",
    "        - Deterministic: $a=\\pi (s)$\n",
    "        - Stochastic: $\\pi[a|s]=P[A|s]$\n",
    "    \n",
    "- <b>Value-based methods</b>: Learns a value function\n",
    "    - $v_{\\pi}(s)$ = $E_{\\pi}(R_{t+1} + R_{t+2} + ... |\\, S_{t}=s)$\n",
    "\n",
    "    - Two types (algos):\n",
    "        - <b>Q learning (traditional RL)</b>\n",
    "        - <b>Deep Q learning (Uses NN)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7504f",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" style=\"width:600px;\" title=\"Q learning vs. Deep Q learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e84d87",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94484c",
   "metadata": {},
   "source": [
    "# Unit 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a167ff7",
   "metadata": {},
   "source": [
    "Unit 2 Key Concepts:<br> \n",
    "<i>RL recap $\\rightarrow$ Value based methods $\\rightarrow$ Greedy policy $\\rightarrow$ Link between value and policy $\\rightarrow$ Epsilon greedy policy $\\rightarrow$ Two main strategies of value based function (state-value function vs. action-value function) $\\rightarrow$ Bellman's equation $\\rightarrow$ Learning strategies (Monte Carlo vs. Temporal Difference) $\\rightarrow$ TD target $\\rightarrow$ Q-learning  $\\rightarrow$ Q-function  $\\rightarrow$ Q-table $\\rightarrow$ Q-learning algorithm $\\rightarrow$ Off-policy vs. On-policy $\\rightarrow$ Q-Learning example $\\rightarrow$ </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625e643",
   "metadata": {},
   "source": [
    "<b>Value based methods</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ce9a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:37:39.559924Z",
     "start_time": "2023-09-05T10:37:39.550095Z"
    }
   },
   "source": [
    "- <b>Greedy Policy</b>: Since policy is not trained/learned in the <b>value-based methods</b>, we define the specific behavior of the policy by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d6c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:33:03.923032Z",
     "start_time": "2023-09-05T10:33:03.912372Z"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" style=\" width:400px; padding: 10px 20px;\" title=\"The link between value and policy\">\n",
    "\n",
    "> <i>Link between value and policy: </i>\n",
    "> $\\pi^{*}(s) = arg \\, \\underset{a}{max} \\, Q^{*}(s,a)$, where\n",
    "> - $\\pi^{*}$ is the optimal policy.\n",
    "> - $Q^{*}$ is the optimal value function.\n",
    "> - $\\underset{a}{max}$ is the pre-defined greedy policy that selects the action that yields the highest expected cumulative value given the state or state action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c822b",
   "metadata": {},
   "source": [
    "- <b>Epsilon-Greedy Policy</b> Policy that handles exploration/exploitation tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0f731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:38:41.055470Z",
     "start_time": "2023-09-05T10:38:41.044070Z"
    }
   },
   "source": [
    "<b>Two main types (strategies) of value based functions:</b>\n",
    "- <b>state-value function, $V$</b>\n",
    "    - $V_{\\pi}(s) = E_{\\pi}[G_{t}|S_{t}=s]$\n",
    "- <b>action-value function, $Q$</b>\n",
    "    - $Q_{\\pi}(s,a) = E_{\\pi}[G_{t}|S_{t}=s, A_{t}=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69386373",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg\" style=\" width:400px; padding: 10px 20px;\" title=\"Bellman equation\">\n",
    "\n",
    "<span style=\"color:red\"><i>Computationally expensive Problem</i>: To claculate each value of a state $V_{\\pi}(s)$ or state-action pair $Q_{\\pi}(s,a)$, we need to sum all the rewards an agent can get if it starts at that state, and followed the policy forever afterwards.</span>\n",
    "\n",
    "<b>Bellman's equation:</b> Simplifies our value estimation.\n",
    "\n",
    "$V_{\\pi}(s) = E[R_{t+1} + \\gamma * V_{\\pi}(S_{t+1})| S_{t}=s)]$ \n",
    "\n",
    "<span style=\"color:blue\">To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, <b>which is a long process</b>, we calculate the value as the <b>sum of immediate reward + the discounted value of the state that follows.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb6985",
   "metadata": {},
   "source": [
    "<b>Learning strategies</b>: How agent will update its policy (or value function) from the experience and reward received during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336b82a",
   "metadata": {},
   "source": [
    "- <b>Monte Carlo</b>: learns at end of each episode (IOW, uses an entire episode of experience before learning).\n",
    "    - $V^{new}(S_{t}) \\leftarrow V(S_{t}) + \\alpha[G_{t}-V(S_{t})]$\n",
    "\n",
    "\n",
    "- <b>Temporal Difference</b>: learns at each step.<br>\n",
    "  <span style=\"color:red\">As we are updating $V(S_{t})$ at each step, we do not have enitre episode of experience, therefore, we don't have $G_{t}$ (expected return)</span>. <span style=\"color:green\"> So, we estimate the expected value using Bellman's equation, and this is called bootstrapping as the <b>TD target</b> is based on estimate $V(S_{t+1})$ and not a complete $G_{t}$.</span><br>\n",
    "  This is also called one-step TD or TD(0).\n",
    "  >In my opinion, TD target is nothing but expected cumulative reward.<br>\n",
    "    \n",
    "    - $V^{new}(S_{t}) \\leftarrow V(S_{t}) + \\alpha[G_{t_{estimate}} - V(S_{t})]$\n",
    "    - $V^{new}(S_{t}) \\leftarrow V(S_{t}) + \\alpha[R_{t+1} + \\gamma * V(S_{t+1}) - V(S_{t})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abca806",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg\" alt=\"Monte Carlo\" style=\"width: 45%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg\" alt=\"Monte Carlo\" style=\"width: 45%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea08a4",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" alt=\"Temporal Difference\" style=\"width: 45%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg\" alt=\"Temporal Difference\" style=\"width: 45%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f061f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T04:47:03.129670Z",
     "start_time": "2023-09-06T04:47:03.125095Z"
    }
   },
   "source": [
    "<b>Q-Learning</b>: An <span style=\"color:blue\"><b><u>off-policy</u> <u>value-based method</u> that uses a <u>TD approach</u> to train its action-value function</b></span>. In other words, an RL algorithm used to train Q-function.\n",
    "\n",
    "<b>Q-Function</b>: This action-value function takes in the state and action as input,  and provides expected value as output. \n",
    "\n",
    "<b>Q-Table</b>: Q-Function is encoded by a Q-table, where each cell corresponds to a state-action pair value. Think of Q-tbale as memory of our Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b966b45",
   "metadata": {},
   "source": [
    "> Let’s recap the difference between value and reward:\n",
    "> - <span style=\"color:blue\">The <i>value of a state, or a state-action pair</i> is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to its policy forever afterwards.</span> In other words, it's the prediction of future rewards.\n",
    "> - <span style=\"color:blue\">The <i>reward</i> is the <b>immediate feedback I get from the environment</b> after performing an action at a state.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd50fe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T05:05:13.268309Z",
     "start_time": "2023-09-06T05:05:13.261069Z"
    }
   },
   "source": [
    "<b>Q-Learning algorithm</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14160a0b",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" style=\"width:800px;\" title=\"Q-learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd4bf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T05:05:13.268309Z",
     "start_time": "2023-09-06T05:05:13.261069Z"
    }
   },
   "source": [
    "- Step 1: We initialise the Q-table arbitrarily.\n",
    "    - $Q(s,a) = 0$ for all $s \\in S$ and $a \\in A(s)$\n",
    "    - $Q(\\text{terminal-state}, .) = 0$\n",
    "- Step 2: <b>Acting/Inference</b>: Choose an action using a policy (<b>epsilon greedy</b> strategy for exploration vs. exploitation trade-off as the training progresses).\n",
    "    - $\\epsilon$ vs. $1-\\epsilon$\n",
    "- Step 3: Peform action $A_{t}$, get reward $R_{t+1}$ and next state $S_{t+1}$.\n",
    "- Step 4: <b>Updating/Training</b>: Update action-value function $Q(S_{t}, A_{t})$ after every step/iteration using <b>greedy</b> policy.\n",
    "    - For state-value function, $V^{new}(S_{t}) \\leftarrow V(S_{t}) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})] $\n",
    "    - For action-value function, $Q^{new}(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha[R_{t+1} + \\gamma \\text{max}_{a}Q(S_{t+1}, a) - Q(S_{t}, A_{t})] $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e5fe5",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg\" alt=\"Q-learning\" style=\"width: 45%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg\" alt=\"Q-learning\" style=\"width: 45%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb3cc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T05:33:57.862204Z",
     "start_time": "2023-09-06T05:33:57.852693Z"
    }
   },
   "source": [
    "<b>Off-policy vs. On-policy</b>: \n",
    "- Off-policy: A different policy for acting (inferencing) and updating (learning)\n",
    "- On-policy: Same policy for acting and inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4482e7",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" style=\"width:600px;\" title=\"Off-on policy\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556981d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "814px",
    "left": "30px",
    "top": "143px",
    "width": "345.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
