{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6ff887",
   "metadata": {},
   "source": [
    "# Unit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b16b3cf",
   "metadata": {},
   "source": [
    "Unit 1 Key concepts:<br> \n",
    "<i>RL Process $\\rightarrow$ MDP $\\rightarrow$ State/Observation Space $\\rightarrow$ Action space $\\rightarrow$ Rewards $\\rightarrow$ Tasks $\\rightarrow$ Exploration/exploitation tradeoff $\\rightarrow$ Policy $\\rightarrow$ Two main approches of solving RL problems $\\rightarrow$ Policy based (Deterministic vs. Stochastic) $\\rightarrow$ Value based $\\rightarrow$ Q learning vs. Deep Q-learning </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536adf3",
   "metadata": {},
   "source": [
    "<b>RL Process</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e43b1",
   "metadata": {},
   "source": [
    "A loop of state, action, reward, and next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded0b0a",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" alt=\"RL process\" style=\"width: 50%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" alt=\"RL loops output sequence\" style=\"width: 30%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7139ec04",
   "metadata": {},
   "source": [
    "<b>Markow's Decision Process</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653376c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T04:58:30.580786Z",
     "start_time": "2023-09-05T04:58:30.570331Z"
    }
   },
   "source": [
    "<b>State/Observation space:</b>\n",
    "- State space\n",
    "- Observation space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b748ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T04:58:56.378653Z",
     "start_time": "2023-09-05T04:58:56.371034Z"
    }
   },
   "source": [
    "<b>Action space</b>\n",
    "- Continuous space\n",
    "- Discrete space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3b3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:03:41.661887Z",
     "start_time": "2023-09-05T05:03:41.642394Z"
    }
   },
   "source": [
    "<b>Reward</b>\n",
    "- Cumulative Reward, $R(\\tau)$\n",
    "- Discounted Expected Cumulative Reward\n",
    "    - Smaller discount => Larger gamma => Agent cares more about long-term reward (and vice versa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c73a59",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg\" style=\"width:400px;\" title=\"Cumulative reward\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg\" style=\"width:150px;\" title=\"Cumulative reward\">\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57719b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:04:37.120278Z",
     "start_time": "2023-09-05T05:04:37.109293Z"
    }
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\" style=\"width:400px;\" title=\"Discounted exepcted cumulative reward\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364bfd7",
   "metadata": {},
   "source": [
    "<b>Tasks</b>\n",
    "- Episodic tasks\n",
    "- Continuous tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d3b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:10:59.035176Z",
     "start_time": "2023-09-05T05:10:59.018090Z"
    }
   },
   "source": [
    "<b>Exploration-exploitation tradeoff</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc87e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T03:01:33.492044Z",
     "start_time": "2023-09-06T03:01:33.462010Z"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" style=\" width:500px; padding: 10px 20px;\" title=\"Value-based Methods\">\n",
    "\n",
    "<b>Policy $\\pi$</b> - Brain of our agent\n",
    "\n",
    "<b>Two main approaches (Policy) for solving RL problems</b>\n",
    "\n",
    "- <b>Policy-based methods</b>: Learns a policy function.\n",
    "    - Two types:\n",
    "        - Deterministic: $a=\\pi (s)$\n",
    "        - Stochastic: $\\pi[a|s]=P[A|s]$\n",
    "    \n",
    "- <b>Value-based methods</b>: Learns a value function\n",
    "    - $v_{\\pi}(s)$ = $E_{\\pi}(R_{t+1} + R_{t+2} + ... |\\, S_{t}=s)$\n",
    "\n",
    "    - Two types (algos):\n",
    "        - <b>Q learning (traditional RL)</b>\n",
    "        - <b>Deep Q learning (Uses NN)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd3c2b",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" style=\"width:600px;\" title=\"Q learning vs. Deep Q learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae472a92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b12c1",
   "metadata": {},
   "source": [
    "# Unit 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f693e",
   "metadata": {},
   "source": [
    "Unit 2 Key Concepts:<br> \n",
    "<i>RL recap $\\rightarrow$ Value based methods $\\rightarrow$ Greedy policy $\\rightarrow$ Link between value and policy $\\rightarrow$ Epsilon greedy policy $\\rightarrow$ Two main strategies of value based function (state-value function vs. action-value function) $\\rightarrow$ Bellman's equation $\\rightarrow$ </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed59cb4",
   "metadata": {},
   "source": [
    "<b>Value based methods</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b346b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:37:39.559924Z",
     "start_time": "2023-09-05T10:37:39.550095Z"
    }
   },
   "source": [
    "- <b>Greedy Policy</b>: Since policy is not trained/learned in the <b>value-based methods</b>, we define the specific behavior of the policy by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb762d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:33:03.923032Z",
     "start_time": "2023-09-05T10:33:03.912372Z"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" style=\" width:400px; padding: 10px 20px;\" title=\"The link between value and policy\">\n",
    "\n",
    "> <i>Link between value and policy: </i>\n",
    "> $\\pi^{*}(s) = arg \\, \\underset{a}{max} \\, Q^{*}(s,a)$, where\n",
    "> - $\\pi^{*}$ is the optimal policy.\n",
    "> - $Q^{*}$ is the optimal value function.\n",
    "> - $\\underset{a}{max}$ is the pre-defined greedy policy that selects the action that yields the highest expected cumulative value given the state or state action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b4794",
   "metadata": {},
   "source": [
    "- <b>Epsilon-Greedy Policy</b> Policy that handles exploration/exploitation tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51390d82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:38:41.055470Z",
     "start_time": "2023-09-05T10:38:41.044070Z"
    }
   },
   "source": [
    "<b>Two main types (strategies) of value based functions:</b>\n",
    "- <b>state-value function, $V$</b>\n",
    "    - $V_{\\pi}(s) = E_{\\pi}[G_{t}|S_{t}=s]$\n",
    "- <b>action-value function, $Q$</b>\n",
    "    - $Q_{\\pi}(s,a) = E_{\\pi}[G_{t}|S_{t}=s, A_{t}=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fdf452",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg\" style=\" width:400px; padding: 10px 20px;\" title=\"Bellman equation\">\n",
    "\n",
    "<span style=\"color:red\"><i>Computationally expensive Problem</i>: To claculate each value of a state $V_{\\pi}(s)$ or state-action pair $Q_{\\pi}(s,a)$, we need to sum all the rewards an agent can get if it starts at that state, and followed the policy forever afterwards.</span>\n",
    "\n",
    "<b>Bellman's equation:</b> Simplifies our value estimation.\n",
    "\n",
    "$V_{\\pi}(s) = E[R_{t+1} + \\gamma * V_{\\pi}(S_{t+1})| S_{t}=s)]$ \n",
    "\n",
    "<span style=\"color:blue\">To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, <b>which is a long process</b>, we calculate the value as the <b>sum of immediate reward + the discounted value of the state that follows.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726304ee",
   "metadata": {},
   "source": [
    "<b>Learning strategies</b>: How agent will update its policy (or value function) from the experience and reward received during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53554109",
   "metadata": {},
   "source": [
    "- <b>Monte Carlo</b>: learns at end of each episode (IOW, uses an entire episode of experience before learning).\n",
    "    - $V(S_{t}) \\leftarrow V(S_{t}) + \\alpha[G_{t}-V(S_{t})]$\n",
    "\n",
    "- <b>Temporal Difference</b>: learns at each step.<br>\n",
    "  <span style=\"color:red\">As we are updating $V(S_{t})$ at each step, we do not have enitre episode of experience, therefore, we don't have $G_{t}$ (expected return)</span>. <span style=\"color:green\"> So, we estimate the expected value using Bellman's equation.</span>\n",
    "    - $V(S_{t}) \\leftarrow V(S_{t}) + \\alpha[G_{t_{estimate}} - V(S_{t})]$\n",
    "    - $V(S_{t}) \\leftarrow V(S_{t}) + \\alpha[R_{t+1} + \\gamma * V(S_{t+1}) - V(S_{t})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e76b5",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg\" alt=\"Monte Carlo\" style=\"width: 40%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg\" alt=\"Monte Carlo\" style=\"width: 40%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca06432",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" alt=\"Temporal Difference\" style=\"width: 40%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg\" alt=\"Temporal Difference\" style=\"width: 40%;\"></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "814px",
    "left": "30px",
    "top": "143px",
    "width": "345.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
