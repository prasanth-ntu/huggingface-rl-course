{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0a3f38",
   "metadata": {},
   "source": [
    "# Unit 1: Introduction to Deep RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefd6a3",
   "metadata": {},
   "source": [
    "Unit 1 Key concepts:<br> \n",
    "<i>RL Process $\\rightarrow$ MDP $\\rightarrow$ State/Observation Space $\\rightarrow$ Action space $\\rightarrow$ Rewards $\\rightarrow$ Tasks $\\rightarrow$ Exploration/exploitation tradeoff $\\rightarrow$ Policy $\\rightarrow$ Two main approches of solving RL problems $\\rightarrow$ Policy based (Deterministic vs. Stochastic) $\\rightarrow$ Value based $\\rightarrow$ Q learning vs. Deep Q-learning </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb99f5",
   "metadata": {},
   "source": [
    "<b>RL Process</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89defd63",
   "metadata": {},
   "source": [
    "A loop of state, action, reward, and next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28781df1",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" alt=\"RL process\" style=\"width: 50%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" alt=\"RL loops output sequence\" style=\"width: 30%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86d3c8",
   "metadata": {},
   "source": [
    "<b>Markow's Decision Process</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d2290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T04:58:30.580786Z",
     "start_time": "2023-09-05T04:58:30.570331Z"
    }
   },
   "source": [
    "<b>State/Observation space:</b>\n",
    "- State space\n",
    "- Observation space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7206dbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T04:58:56.378653Z",
     "start_time": "2023-09-05T04:58:56.371034Z"
    }
   },
   "source": [
    "<b>Action space</b>\n",
    "- Continuous space\n",
    "- Discrete space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1c3d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:03:41.661887Z",
     "start_time": "2023-09-05T05:03:41.642394Z"
    }
   },
   "source": [
    "<b>Reward</b>\n",
    "- Cumulative Reward, $R(\\tau)$\n",
    "- Discounted Expected Cumulative Reward\n",
    "    - Smaller discount => Larger gamma => Agent cares more about long-term reward (and vice versa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1eb2d8",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg\" style=\"width:400px;\" title=\"Cumulative reward\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg\" style=\"width:150px;\" title=\"Cumulative reward\">\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1f227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:04:37.120278Z",
     "start_time": "2023-09-05T05:04:37.109293Z"
    }
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\" style=\"width:400px;\" title=\"Discounted exepcted cumulative reward\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b05c0f",
   "metadata": {},
   "source": [
    "<b>Tasks</b>\n",
    "- Episodic tasks\n",
    "- Continuous tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2036795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T05:10:59.035176Z",
     "start_time": "2023-09-05T05:10:59.018090Z"
    }
   },
   "source": [
    "<b>Exploration-exploitation tradeoff</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb5ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T03:01:33.492044Z",
     "start_time": "2023-09-06T03:01:33.462010Z"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" style=\" width:500px; padding: 10px 20px;\" title=\"Value-based Methods\">\n",
    "\n",
    "<b>Policy $\\pi$</b> - Brain of our agent\n",
    "\n",
    "<b>Two main approaches (Policy) for solving RL problems</b>\n",
    "\n",
    "- <b>Policy-based methods</b>: Learns a policy function.\n",
    "    - Two types:\n",
    "        - Deterministic: $a=\\pi (s)$\n",
    "        - Stochastic: $\\pi[a|s]=P[A|s]$\n",
    "    \n",
    "- <b>Value-based methods</b>: Learns a value function\n",
    "    - $v_{\\pi}(s)$ = $E_{\\pi}(R_{t+1} + R_{t+2} + ... |\\, S_{t}=s)$\n",
    "\n",
    "    - Two types (algos):\n",
    "        - <b>Q learning (traditional RL)</b>\n",
    "        - <b>Deep Q learning (Uses NN)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d2bf1",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" style=\"width:600px;\" title=\"Q learning vs. Deep Q learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a280719",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b06f13",
   "metadata": {},
   "source": [
    "# Unit 2: Introduction to Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ec69f",
   "metadata": {},
   "source": [
    "Unit 2 Key Concepts:<br> \n",
    "<i>RL recap $\\rightarrow$ Value based methods $\\rightarrow$ Greedy policy $\\rightarrow$ Link between value and policy $\\rightarrow$ Epsilon greedy policy $\\rightarrow$ Two main strategies of value based function (state-value function vs. action-value function) $\\rightarrow$ Bellman's equation $\\rightarrow$ Learning strategies (Monte Carlo vs. Temporal Difference) $\\rightarrow$ TD target $\\rightarrow$ Q-learning  $\\rightarrow$ Q-function  $\\rightarrow$ Q-table $\\rightarrow$ Q-learning algorithm $\\rightarrow$ Off-policy vs. On-policy $\\rightarrow$ Q-Learning example </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74061997",
   "metadata": {},
   "source": [
    "<b>Value based methods</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5942c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:37:39.559924Z",
     "start_time": "2023-09-05T10:37:39.550095Z"
    }
   },
   "source": [
    "- <b>Greedy Policy</b>: Since policy is not trained/learned in the <b>value-based methods</b>, we define the specific behavior of the policy by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a81f4d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:33:03.923032Z",
     "start_time": "2023-09-05T10:33:03.912372Z"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" style=\" width:400px; padding: 10px 20px;\" title=\"The link between value and policy\">\n",
    "\n",
    "> <i>Link between value and policy: </i>\n",
    "> $\\pi^{*}(s) = arg \\, \\underset{a}{max} \\, Q^{*}(s,a)$, where\n",
    "> - $\\pi^{*}$ is the optimal policy.\n",
    "> - $Q^{*}$ is the optimal value function.\n",
    "> - $\\underset{a}{max}$ is the pre-defined greedy policy that selects the action that yields the highest expected cumulative value given the state or state action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289fd60",
   "metadata": {},
   "source": [
    "- <b>Epsilon-Greedy Policy</b> Policy that handles exploration/exploitation tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4c28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T10:38:41.055470Z",
     "start_time": "2023-09-05T10:38:41.044070Z"
    }
   },
   "source": [
    "<b>Two main types (strategies) of value based functions:</b>\n",
    "- <b>state-value function, $V$</b>\n",
    "    - $V_{\\pi}(s) = E_{\\pi}[G_{t}|S_{t}=s]$\n",
    "- <b>action-value function, $Q$</b>\n",
    "    - $Q_{\\pi}(s,a) = E_{\\pi}[G_{t}|S_{t}=s, A_{t}=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b667f9e",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg\" style=\" width:400px; padding: 10px 20px;\" title=\"Bellman equation\">\n",
    "\n",
    "<span style=\"color:red\"><i>Computationally expensive Problem</i>: To claculate each value of a state $V_{\\pi}(s)$ or state-action pair $Q_{\\pi}(s,a)$, we need to sum all the rewards an agent can get if it starts at that state, and followed the policy forever afterwards.</span>\n",
    "\n",
    "<b>Bellman's equation:</b> Simplifies our value estimation.\n",
    "\n",
    "$V_{\\pi}(s) = E[R_{t+1} + \\gamma * V_{\\pi}(S_{t+1})| S_{t}=s)]$ \n",
    "\n",
    "<span style=\"color:blue\">To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, <b>which is a long process</b>, we calculate the value as the <b>sum of immediate reward + the discounted value of the state that follows.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ffd2c6",
   "metadata": {},
   "source": [
    "<b>Learning strategies</b>: How agent will update its policy (or value function) from the experience and reward received during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394731fa",
   "metadata": {},
   "source": [
    "- <b>Monte Carlo</b>: learns at end of each episode (IOW, uses an entire episode of experience before learning).\n",
    "    - $V^{new}(S_{t}) \\leftarrow V(S_{t}) + \\alpha[G_{t}-V(S_{t})]$\n",
    "\n",
    "\n",
    "- <b>Temporal Difference (TD)</b>: learns at each step.<br>\n",
    "  <span style=\"color:red\">As we are updating $V(S_{t})$ at each step, we do not have enitre episode of experience, therefore, we don't have $G_{t}$ (expected return)</span>. <span style=\"color:green\"> So, we estimate the expected value using Bellman's equation, and this is called bootstrapping as the <b>TD target</b> is based on estimate $V(S_{t+1})$ and not a complete $G_{t}$.</span><br>\n",
    "  This is also called one-step TD or TD(0).\n",
    "  >In my opinion, TD target is nothing but expected cumulative reward.<br>\n",
    "    \n",
    "    - $V^{new}(S_{t}) \\leftarrow V(S_{t}) + \\alpha[G_{t_{estimate}} - V(S_{t})]$\n",
    "    - $V^{new}(S_{t}) \\leftarrow V(S_{t}) + \\alpha[R_{t+1} + \\gamma * V(S_{t+1}) - V(S_{t})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a409824",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg\" alt=\"Monte Carlo\" style=\"width: 45%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg\" alt=\"Monte Carlo\" style=\"width: 45%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b140b1",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" alt=\"Temporal Difference\" style=\"width: 45%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg\" alt=\"Temporal Difference\" style=\"width: 45%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec827e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T04:47:03.129670Z",
     "start_time": "2023-09-06T04:47:03.125095Z"
    }
   },
   "source": [
    "<b>Q-Learning</b>: An <span style=\"color:blue\"><b><u>off-policy</u> <u>value-based method</u> that uses a <u>TD approach</u> to train its action-value function</b></span>. In other words, an RL algorithm used to train Q-function.\n",
    "\n",
    "<b>Q-Function</b>: This action-value function takes in the state and action as input,  and provides expected value as output. \n",
    "\n",
    "<b>Q-Table</b>: Q-Function is encoded by a Q-table, where each cell corresponds to a state-action pair value. Think of Q-tbale as memory of our Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244fa58",
   "metadata": {},
   "source": [
    "> Let’s recap the difference between value and reward:\n",
    "> - <span style=\"color:blue\">The <i>value of a state, or a state-action pair</i> is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to its policy forever afterwards.</span> In other words, it's the prediction of future rewards.\n",
    "> - <span style=\"color:blue\">The <i>reward</i> is the <b>immediate feedback I get from the environment</b> after performing an action at a state.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640d009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T05:05:13.268309Z",
     "start_time": "2023-09-06T05:05:13.261069Z"
    }
   },
   "source": [
    "<b>Q-Learning algorithm</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575822d2",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" style=\"width:700px;\" title=\"Q-learning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ee7ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T05:05:13.268309Z",
     "start_time": "2023-09-06T05:05:13.261069Z"
    }
   },
   "source": [
    "- Step 1: We initialise the Q-table arbitrarily.\n",
    "    - $Q(s,a) = 0$ for all $s \\in S$ and $a \\in A(s)$\n",
    "    - $Q(\\text{terminal-state}, .) = 0$\n",
    "- Step 2: <b>Acting/Inference</b>: Choose an action using a policy (<b>epsilon greedy</b> strategy for exploration vs. exploitation trade-off as the training progresses).\n",
    "    - $\\epsilon$ vs. $1-\\epsilon$\n",
    "- Step 3: Peform action $A_{t}$, get reward $R_{t+1}$ and next state $S_{t+1}$.\n",
    "- Step 4: <b>Updating/Training</b>: Update action-value function $Q(S_{t}, A_{t})$ after every step/iteration using <b>greedy</b> policy.\n",
    "    - For state-value function, $V^{new}(S_{t}) \\leftarrow V(S_{t}) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})] $\n",
    "    - For action-value function, $Q^{new}(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha[R_{t+1} + \\gamma \\text{max}_{a}Q(S_{t+1}, a) - Q(S_{t}, A_{t})] $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909be9c",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg\" alt=\"Q-learning\" style=\"width: 40%;\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg\" alt=\"Q-learning\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950d177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T05:33:57.862204Z",
     "start_time": "2023-09-06T05:33:57.852693Z"
    }
   },
   "source": [
    "<b>Off-policy vs. On-policy</b>: \n",
    "- Off-policy: A different policy for acting (inferencing) and updating (learning)\n",
    "- On-policy: Same policy for acting and inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a903ef",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" style=\"width:600px;\" title=\"Off-on policy\">\n",
    "\n",
    "Also, the greedy policy will also be the final policy we'll have when the Q-learning agent completes training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546710f",
   "metadata": {},
   "source": [
    "# Unit 3: Deep Q-learning with Atari Games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8884382",
   "metadata": {},
   "source": [
    "Unit 3 Key Concepts:<br> \n",
    "<i> From Q-learning to Deep Q-learning $\\rightarrow$ Deep Q-Network $\\rightarrow$ Preprocessing the input $\\rightarrow$ Temporal limitation $\\rightarrow$ $\\rightarrow$ $\\rightarrow$ $\\rightarrow$ </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c04bfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T12:43:16.780968Z",
     "start_time": "2023-09-06T12:43:16.776113Z"
    }
   },
   "source": [
    "<b>State spaces</b>\n",
    "- LunarLander-v2 - ? different states.\n",
    "- FrozenLake-v1 - 16 different states (4x4 grid).\n",
    "- Taxi-v3 - 500 different states (5x5 grid) x (5 passenger locations) x (4 destination locations).\n",
    "- Atari - 10^9 to 10^11 states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a43f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T12:43:44.875469Z",
     "start_time": "2023-09-06T12:43:44.861772Z"
    }
   },
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" style=\" width:550px; padding: 10px 20px;\" title=\"Deep Q Learning\">\n",
    "\n",
    "<b>From Q-learning to Deep Q-learning</b>\n",
    "\n",
    "<span style=\"color:red\"><b>The Q-table becomes ineffective in large state space environments</b></span>, though it worked well for smaller discrete state spaces like LunarLander-v2 and FrozenLake-v1. So, instead of using a Q-table, we need to use <span style=\"color:green\">Deep Q-Learning that uses a Neural Network to approximate, given a state, different Q-values for each possible action based on that state. In other words, <span style=\"color:green\"> Approximate Q-values using a parameterised Q-function $Q_{\\theta}(s,a)$</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b6725a",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg\" style=\" width:500px; padding: 10px 20px;\" title=\"Deep Q Network\">\n",
    "\n",
    "<b>The Deep Q-Network</b>\n",
    "- Input: Stack of 4 frames\n",
    "- Output: Vector of Q values for each possible action at that state\n",
    "\n",
    "<b>Proprocessing the input</b>\n",
    "- To reduce state space complexity => Faster training\n",
    "- <span style=\"color:blue\"><b>16x reduction</b> from $\\Rightarrow 210*160*3=113,400$ pixels in a frame to $84*84*1=7,056$ pixels.</span>\n",
    "\n",
    "<b>Temporal Limitation</b>\n",
    "- Handled by stacking multiple frames together, thereby capturing <b>temporal information</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c289036b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T07:18:51.363403Z",
     "start_time": "2023-09-18T07:18:51.349616Z"
    }
   },
   "source": [
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe1cc8",
   "metadata": {},
   "source": [
    "# Bous Unit 2: Automatic Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9c951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "814px",
    "left": "55px",
    "top": "111.141px",
    "width": "345.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
