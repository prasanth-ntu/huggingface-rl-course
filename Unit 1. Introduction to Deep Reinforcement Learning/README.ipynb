{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfe7928",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/what-is-rl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845dbc31",
   "metadata": {},
   "source": [
    "Deep RL is a type of Machine Learning where an agent learns <b>how to behave</b> in an environment <b>by performing actions and seeing the results</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38baf756",
   "metadata": {},
   "source": [
    "# What is Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e9005",
   "metadata": {},
   "source": [
    "## The big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7ea57",
   "metadata": {},
   "source": [
    "### The formal definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a4111",
   "metadata": {},
   "source": [
    "> Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28faa0b",
   "metadata": {},
   "source": [
    "# The Reinforcement Learning Framework\n",
    "\n",
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/rl-framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7378168",
   "metadata": {},
   "source": [
    "## The RL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d9b4f",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" style=\"width:600px;\" title=\"RL process\">\n",
    "\n",
    "A loop of state, action, reward, and next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce68b0",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" style=\"width:300px;\" title=\"RL loops outputs sequuence\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e57530",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size: large\">The agent’s goal is to <i>maximize</i> its cumulative reward, called the <b>expected return</b>.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeab1c3",
   "metadata": {},
   "source": [
    "## The reward hypothesis: the central idea of reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa7af3",
   "metadata": {},
   "source": [
    "## Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174be61",
   "metadata": {},
   "source": [
    "In papers, you’ll see that the RL process is called a <b>Markov Decision Process</b> (MDP).\n",
    "\n",
    "The Markov Property implies that our agent needs <b>only the current state to decide</b> what action to take and <b>not the history of all the states and actions</b> they took before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ace4a",
   "metadata": {},
   "source": [
    "## Observations/States Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48bd2b",
   "metadata": {},
   "source": [
    "Observations/States are the <b>information our agent gets from the environment</b>. For e.g., frame in a video game, value of certain stock in trading.\n",
    "\n",
    "- State $s$: Fully observed environment\n",
    "    - Chess\n",
    "- Observation $o$: Partially observed environment\n",
    "    - Super mario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431a71e",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg\" style=\"width:600px;\" title=\"Observation vs State space\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b8dfa",
   "metadata": {},
   "source": [
    "## Action Space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da0b6e26",
   "metadata": {},
   "source": [
    "The Action space is the set of <b>all possible actions in an environment</b>.\n",
    "\n",
    "- Discrete space: the number of possible actions is finite.\n",
    "    - e.g., Super mario\n",
    "- Continuous space: the number of possible actions is infinite.\n",
    "    - e.g., Self Driving car\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg\" style=\"width:600px;\" title=\"Action space\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2c4c9",
   "metadata": {},
   "source": [
    "## Rewards and the discounting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4344e9ca",
   "metadata": {},
   "source": [
    "<i>Cumulative reward = Sum of all rewards in the sequence<i>\n",
    "    \n",
    "The cumulative reward at each time step $t$ can be written as:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg\" style=\"width:500px;\" title=\"Cumulative reward\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg\" style=\"width:200px;\" title=\"Cumulative reward\">\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6217f",
   "metadata": {},
   "source": [
    "However, in reality, <b>we can’t just add them like that</b>. The rewards that come sooner (at the beginning of the game) <b>are more likely to happen</b> since they are more predictable than the long-term future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3931a",
   "metadata": {},
   "source": [
    "Our discounted expected cumulative reward is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40070e6b",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\" style=\"width:500px;\" title=\"Discounted exepcted cumulative reward\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bd1bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T01:39:06.583985Z",
     "start_time": "2023-08-12T01:39:06.571697Z"
    }
   },
   "source": [
    "where\n",
    "- $\\gamma$ is discount rate, <b>between 0 and 1</b>\n",
    "    - Larger the $\\gamma$, smaller the discount => Agent <b>cares more about long-term reward.</b>\n",
    "    - Smaller the $\\gamma$, larger the discount => Agent <b>cares more abotu the short-term reward.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2aef4",
   "metadata": {},
   "source": [
    "# The type of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a264354",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/tasks\n",
    "\n",
    "A task is an <b>instance</b> of a Reinforcement Learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596f4ce",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/tasks.jpg\" style=\"width:600px;\" title=\"Type of tasks\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811153a9",
   "metadata": {},
   "source": [
    "## Episodic task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dce0b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T01:43:52.172604Z",
     "start_time": "2023-08-12T01:43:52.163282Z"
    }
   },
   "source": [
    "- e.g., Super Mario Bros\n",
    "    - An episode begins at launch of a new level and ends <b>when we are killed or reach end of the level<b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24caa4bb",
   "metadata": {},
   "source": [
    "## Continuous task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f8958",
   "metadata": {},
   "source": [
    "- e.g., Automated Stock trading\n",
    "    - There's no starting point and ending point $\\Rightarrow$ No terminal state. <b>The agent keeps running till we decide to stop it.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca32a3",
   "metadata": {},
   "source": [
    "# The Exploration/ Exploitation tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1cbdd",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/exp-exp-tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53a060",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg\" style=\"width:600px;\" title=\"Exploration Exploitation Tradeoff\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e50d3",
   "metadata": {},
   "source": [
    "We need to balance how much we <b>explore the environment</b> and how much we <b>exploit what we know about the environment</b>.\n",
    "\n",
    "Therefore, we must <b>define a rule that helps to handle this trade-off</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5c34e",
   "metadata": {},
   "source": [
    "# The two main approaches for solving RL problems\n",
    "\n",
    "Source: https://huggingface.co/learn/deep-rl-course/unit1/two-methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa20dad",
   "metadata": {},
   "source": [
    "## The policy π: the agent’s brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd105f22",
   "metadata": {},
   "source": [
    "The Policy <b>π</b> is the <b>brain of our Agent</b>, it’s the <i>function that tells us what <b>action to take given the state</b></i>. So it <b>defines the agent’s behavior</b> at a given time.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_1.jpg\" style=\"width:300px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847863f",
   "metadata": {},
   "source": [
    "| Policy-based | Value-based | \n",
    "| :-: | :-: |\n",
    "| <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_1.jpg\" style=\"width:450px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\"> <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_2.jpg\" style=\"width:450px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\"> |  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_1.jpg\" style=\"width:450px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\"> <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_2.jpg\" style=\"width:450px;\" title=\"Policy - The brain of agent, a function that tells us the action to take given the state.\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc474e13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T01:48:59.040231Z",
     "start_time": "2023-08-12T01:48:59.032518Z"
    }
   },
   "source": [
    "## Policy-Based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c67e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d835401",
   "metadata": {},
   "source": [
    "## Value-Based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f12e3f",
   "metadata": {},
   "source": [
    "# The “Deep” in Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39087399",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9788396",
   "metadata": {},
   "source": [
    "# Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee2ba9",
   "metadata": {},
   "source": [
    "# Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db05dd",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f37a0",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf3185",
   "metadata": {},
   "source": [
    "# Additional Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063baff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_related",
   "language": "python",
   "name": "fastai_related"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
