| Term | Description |
| - | - |
| Action space | Set of all possible actions in an [environment](#environment). <br>Two types: <br>- [Discrete space](#discrete-space)<br>- [Continuous space](#continuous-space)  |
| <a id="action-value-function"></a>Action-value function | For each state and action pair, the action-value function outputs the expected reutrn if the agent <b>starts at that state, takes an action</b>, and then follows the policy forever after. |  
| <a id="agent"></a>Agent | To make intelligent decisions, learns from the [environment](#environment) by interacting with it through trial and error and receiving [rewards](#reward) (positive or negative) as unique feedback.<br> Agent's goal is to maximise the [cumulative reward](#cumulative-reward). | 
| Bellman equation | Simplifies our state value or action-value calculation.<br> Instead of calculating each value as the sum of the expected return (i.e., sum of all the rewards if agent starts at that state), <b>which is a long process</b>, we calculate the value as the <b>sum of immediate reward + the discounted value of the state that follows<b>.|
| <a id="continuous-task"></a>Continuous tasks | Tasks that has a starting point, but no ending point. |
| <a id="continuous-space"></a>Continuous space | Number of possible actions is infinite. <br>e.g., Self driving car. |
| <a id="cumulative-reward"> Cumulative reward | Sum of all rewards in a sequence. <br> Variant: Diccounted expected cumulative reward.|
| Deep Q Algorithm | The Deep Q-learning training algorithm has <i>two phases</i>.<br> 1. <b>Sampling</b>: we perform actions and </b>store the observed experience tuples in a <b>replay memory. <br>2. <b>Training</b>: Select a <b>small batch of tuples randomly and learn from this batch using a gradient descent update step</b>. |
| <a id="deep-q-learning"></a>Deep Q-Learning (DQN) | Instead of using a [Q-table](#q-table), deep Q-learning uses a Neural Network that takes a state and approximates Q-values for each action based on that state.<br><br> To stabilise the training, <br> 1. <i>[Experience Replay](#experience-replay)</i> to make more <b>efficient use of experiences</b>. <br>2. <i>[Fixed Q-Target](#fixed-q-target)</i> to <b>stabilize the training</b>. <br> 3. <i>[Double Deep Q-Learning](#double-deep-q-learning)</i>, to <b>handle the problem of the overestimation of Q-values</b>. |
| Deep Q Network | |
| Deep RL | Introduced deep neural networks to [RL](#RL) problems, hence the name "deep". |
| <a id="discount"></a>Discount (rate) | Performed because rewards obtained at the start are more likely to happen as they are more predictable than long-term rewards. <br> - Smaller discount (larger gamma) ⇒ Agent cares more about long-term reward. <br> - Larger discount (smaller gamma) ⇒ Agent cares more about short-term reward. |
| <a id="discrete-space"></a>Discrete space | Number of possible actions is finite. <br>e.g., Super Mario Game. |
| <a id="double-deep-q-learning"></a> Double Deep Q-Learning (Double DQN)| When we compute the Q target, we use two networks to decouple the action selection (network 1) from the target Q-value generation (network 2). |
| <a id="environment"></a>Environment | Simulated world where an <b>agent can learn by interacting with it</b>. <br>Used in the course:<br>- [LunarLander-v2](#lunarlander-v2) |
| <a id="episodic-task"></a>Episodic tasks | Has a starting point and an ending point.<br>e.g., Automated stock trading. |
| <a id="epsilon-greedy-policy"></a>Epsilon-greedy policy | Policy that handles the exploration/exploitation trade-off. <br> Epsilon is typically decreased over time (training) to shift focus from exploration to exploitation. | 
| <a id="experience-replay"></a>Experience Replay | 1. <b>Make more efficient use of the experiences during the training </b>⇒ This allows the agent to learn from the same experiences multiple times. <br> 2. <b>Avoid forgetting previous experiences and reduce the correlation between experiences.</b>|
| <a id="exploitation"></a>Exploitation | |
| <a id="exploration"></a>Exploration | |
| <a id="fixed-q-target"></a>Fixed Q-Target| 1. Use a <b>separate network with fixed parameters</b> for estimating the TD Target.<br> 2. <b>Copy the parameters from our Deep Q-Network every C steps</b> to update the target network. |
| <a id="frozenlake-v1">FrozenLake-v1 | A game environment. It involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. | 
| Greedy policy | Policy where the agent always chooses the action with the highest estimated reward, based on the current knowledge of the environment (i.e., from the value function in the value based methods(CMIIW)?) (=> Only [exploitation](#exploitation), and no [exploration](#exploration)). While this policy may seem optimal, it often isn't, because the agent may get stuck in a local maximum and stop exploring potentially higher-reward actions.  <br> Variants: <br>- [Epsilon-greedy policy](#epsilon-greedy-policy) | 
| <a id="GLIE"></a> Greedy in the limit of Infinite Exploration (GLIE) | A simple GLIE strategy is [Epsilon-greedy policy](#epsilon-greedy-policy), where ε is reduced to 0 with following rate: ε =1/i, where i reresents number of episodes. |
| Gymnasium | The Gymnasium library provides two things:<br>1. An interface that allows us to create RL environments.<br>2. A collection of [environments](#environment) (gym-control, atari, box2D...). |
| Learning strategies | <b>Strategies on how to train our value function or policy function</b>. <b>Uses  experience to solve the RL problem. </b><br> Two learning strategies:<br> - [Monte-Carlo](#monte-carlo-learning)<br> - [Temporal Difference](#temporal-difference-learning) |
| <a id="lunarlander-v2"></a>[LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/) | Environment is a classic rocket trajectory optimization problem. Used in the course. |
| <a id="markov-decision-process"></a>Markov Decision Process (MDP) | Markow property implies that our [agent](#agent) only needs the current state to decide what action to take and <b>not the history of all the states and actions they took before.</b> | 
| <a id="monte-carlo-learning"></a>Monte Carlo (MC) Learning | Uses <b>an entire episode of experience before learning</b>  (i.e., update the value function or policy function). |
| Observation/State | <b>Information our agent gets from the (fully or partially observed) environment</b> |
| Observation space | <b>Partial description</b> of the state of the world.<br>e.g., Frame in a Super Mario game |
| <a id="off-policy"></a>Off-policy | Using <b>a different policy for acting (inference) and updating (training)</b>. <br> Example: [Q-learning](#q-learning). |
| <a id="on-policy"></a>On-policy | Using the <b>same policy for acting (inference) and updating (training)</b>. <br>Example: SARSA learning.|
| <a id="policy"></a>Policy | <b>Agent's brain (or) decision-making process</b>. Tells the agent what action to take, given a state. So, it <b>defines the agent's behavior at a given time</b>. <br>Our goal in RL is to find the Optimal policy that <b>gives us the actions that maximises the expected (cumulatuve) return</b>.<br>Two ways to find optimal policy: [Policy-based](#policy-based), [Value-based](#value-based) |
| <a id="policy-based"></a>Policy-based methods | Trains a policy directly to <b>learn which action to take given a state</b>. <br> - Input: State <br>- Output: Action to take at that state. <br>Two types: Deterministic, Stochastic.| 
| <a id="PPO"></a> Proximal Policy Optimization (PPO) | A combination of <br>- [Value based](#value-based) RL method - learning an action-value function that will tell us the most valuable action to take given a state and action. <br>- [Policy based](#policy-based) RL method - learning a policy that will give us a probability distribution over actions. |
| <a id="q-function"></a>Q-Function | An <b>[action-value function](#action-value-function)</b> that determines the value of being at a particular state and taking a specific action at that state. <br> Q-function is encoded by a [Q-table](#q-table). Given a state and a function, Q-function will search its Q-table for the corresponding value. <br> The <b>Q comes from the "Quality" of that action at that state.</b> |
| <a id="q-learning"></a>Q-Learning |<b>[Off-policy](#off-policy) value-based method that uses a [TD](#temporal-difference-learning) approach to train its [action-value function](#action-value-function)</b>.<br> An <b>RL algorithm we use to train [Q-function](#q-function).</b><br><br> Works well with small state space environments like `FrozenLake` (16 states), `Taxi-v3` (500 states). However, it's not scalable for states and action spaces that are not small enough. For such cases, we have to use [Deep Q-Learning](#deep-q-learning). | 
| <a id="q-table"></a>Q-Table | A <b>table where each cell corresponds to state-action pair</b>. Think of it like a <b>memory or a cheat-sheet of our [Q-function](#q-function)</b>. | 
| <a id="reward"></a>Reward | Fundamental factor in RL. The design of the reward function is crucial in RL as it tells what agent should aim and whether the action taken is good/bad. A well-designed reward function can guide the agent to learn the desired behaviour effectively. <br><b>It's the feedback we get from the environment</b> after performing an action at a state. |
| Reward hypothesis | [RL](#RL) problems can be formulated as a maximization of the (cumulative) reward. In simpler words, <b>every goal that an agent might have can be framed as a problem of maximizing the cumulative reward it receives over time.</b> | 
| <a id="RL"></a>RL |  A framework for solving decision-making problems.<br>Build [agents](#agent) that can make smart decisions. <br>RL algorithms are focused on <b>maximizing the [cumulative reward](#cumulative-reward)</b>. <br> To solve an RL problem, we need to <b>find an optimal [policy](#policy) π</b>, that tells <b>what action to take given a state</b>.| 
| <a id="rl-zoo"></a> RL-Zoo (or) [RL Baselines3 Zoo](https://stable-baselines3.readthedocs.io/en/master/) | A training framework for RL using Stable-Baselines that provide script for training, evaluating agents, tuning hyperparameters, plotting results, and recording videos.  |
| <a id="state-space"></a>State space | <b>Complete description</b> of the state of the world (no hidden information).<br>e.g., <br>- Chess states<br>- [FrozenLake-v1](#frozenlake-v1) - 16 different states. <br> - [Taxi-v3](#taxi-v3) - 500 different states. <br> - Atari - 10^9 to 10^11 states. |
| <a id="state-value-function"></a>State-value function | For each state, the state-value function outputs the exepcted return if the agent <b>starts at that state</b> and then follows the policy forever afterward.|
| <a id="tasks"></a>Tasks | An instance of a [RL](#RL).<br>Two types:<br>- [Episodic task](#episodic-task).<br>- [Continous task](#continuous-task). |
| <a id="temporal-difference-learning"></a>Temporal Difference (TD) Learning| Uses <b>only a step (one interaction) to learn</b> (i.e., update the value function), instead of entire episode like [Monte Carlo learning](#monte-carlo-learning).<br>In my opinion, TD target is nothing but expected cumulative reward.<br> This method is also called one-step TD or TD(0). |  
| Value of a state or a state-action pair| Expected cumulative [reward](#reward) agent gets if it starts at this state (or state-action pair) nad then acts accordingly to its policy. |
|  <a id="value-based"></a>Value-based methods | Trains a value function to <b>learn which states are more valuable</b> and use this value function to <b>take action that leads to it</b>. <br><br><b>Two value based algos</b>: <br> - Q-Learning (classic RL), <br> - Deep Q-Learning (Uses NN). <br><br> <b>Two main types (strategies) of value based functions</b>:<br>- [State value function](#state-value-function): Calculates value of a state.<br> - [Action value function](#action-value-function): Calculates value of state-action pair. |
| | |