| Term | Description |
| - | - |
| Agent | Learns from the environment by interacting with it through trial and error and receiving [rewards](#reward) (positive or negative) as unique feedback. | 
| Policy-based methods | Trains a policy directly to <b>learn which action to take given a state</b>. | 
| <a id="reward"></a>Reward | Fundamental factor in RL. Tells agent whether the action taken is good/bad. |
| Reward hypothesis | RL problems can be formulated as a maximization of the (cumulative) reward | 
| RL |  Build agents that can make smart decisions. <br>RL algorithms are focused on <b>maximizing the cumulative reward</b>. | 
| Value-based methods | Trains a value function to <b>learn which states are more valuable</b> and use this value function to <b>take action that leads to it</b> |
