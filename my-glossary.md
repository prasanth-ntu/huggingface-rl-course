| Term | Description |
| - | - |
| <a id="agent"></a>Agent | To make intelligent decisions, learns from the environment by interacting with it through trial and error and receiving [rewards](#reward) (positive or negative) as unique feedback. | 
| <a id="policy"></a>Policy | <b>Agent's brain (or) decision-making process<b>. Tells the agent what action to take, given a state. |
| Policy-based methods | Trains a policy directly to <b>learn which action to take given a state</b>. | 
| <a id="reward"></a>Reward | Fundamental factor in RL. Tells agent whether the action taken is good/bad. |
| Reward hypothesis | [RL](#RL) problems can be formulated as a maximization of the (cumulative) reward | 
| <a id="RL"></a>RL |  Build [agents](#agent) that can make smart decisions. <br>RL algorithms are focused on <b>maximizing the cumulative reward</b>. | 
| Value-based methods | Trains a value function to <b>learn which states are more valuable</b> and use this value function to <b>take action that leads to it</b> |
