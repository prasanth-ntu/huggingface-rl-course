{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fa8c33",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#From-single-agent-to-multiple-agents\" data-toc-modified-id=\"From-single-agent-to-multiple-agents-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>From single agent to multiple agents</a></span></li><li><span><a href=\"#Different-types-of-multi-agent-environments\" data-toc-modified-id=\"Different-types-of-multi-agent-environments-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Different types of multi-agent environments</a></span></li></ul></li><li><span><a href=\"#What-are-the-policy-based-methods?\" data-toc-modified-id=\"What-are-the-policy-based-methods?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What are the policy-based methods?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Value-based,-Policy-based,-and-Actor-critic-methods\" data-toc-modified-id=\"Value-based,-Policy-based,-and-Actor-critic-methods-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Value-based, Policy-based, and Actor-critic methods</a></span></li></ul></li><li><span><a href=\"#The-advantages-and-disadvantages-of-policy-gradient-methods\" data-toc-modified-id=\"The-advantages-and-disadvantages-of-policy-gradient-methods-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The advantages and disadvantages of policy-gradient methods</a></span></li><li><span><a href=\"#Diving-deeper-into-policy-gradient\" data-toc-modified-id=\"Diving-deeper-into-policy-gradient-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Diving deeper into policy-gradient</a></span></li><li><span><a href=\"#(Optional)-the-Policy-Gradient-Theorem\" data-toc-modified-id=\"(Optional)-the-Policy-Gradient-Theorem-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>(Optional) the Policy Gradient Theorem</a></span></li><li><span><a href=\"#Glossary\" data-toc-modified-id=\"Glossary-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Glossary</a></span></li><li><span><a href=\"#Hands-on\" data-toc-modified-id=\"Hands-on-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Hands-on</a></span></li><li><span><a href=\"#Quiz\" data-toc-modified-id=\"Quiz-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Quiz</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Additional-Readings\" data-toc-modified-id=\"Additional-Readings-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Additional Readings</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b6696",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d64ada",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a3df3",
   "metadata": {},
   "source": [
    "## From single agent to multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f1611",
   "metadata": {},
   "source": [
    "In the first unit, we learned to train agents in a single-agent system. When our agent was alone in its environment: <b>it was not cooperating or collaborating with other agents.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7ca470",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/patchwork.jpg\" style=\"width:600px;\" title=\"Patchwork\">\n",
    "<span style=\"font-size:small\"><center><i>A patchwork of all the environments you've trained your agents on since the beginning of the course.</i></center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ff8b0",
   "metadata": {},
   "source": [
    "When we do multi-agents reinforcement learning (MARL), we are in a situation where we have multiple agents <b>that share and interact in a common environment.</b>\n",
    "\n",
    "For instance, you can think of a warehouse where <b>multiple robots need to navigate to load and unload packages.</b>\n",
    "\n",
    "Or a road with <b>several autonomous vehicles.</b>\n",
    "\n",
    "In these examples, we have <b>multiple agents interacting in the environment and with the other agents</b>. This implies defining a multi-agents system. But first, let’s understand the different types of multi-agent environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2707f4af",
   "metadata": {},
   "source": [
    "## Different types of multi-agent environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1efa09",
   "metadata": {},
   "source": [
    "Given that, in a multi-agent system, agents interact with other agents, we can have different types of environments:\n",
    "\n",
    "- <i>Cooperative environments</i>: where your agents need <b>to maximize the common benefits.</b>\n",
    "\n",
    "For instance, in a warehouse, <b>robots must collaborate to load and unload the packages efficiently (as fast as possible).</b>\n",
    "\n",
    "- <i>Competitive/Adversarial environments</i>: in this case, your agent <b>wants to maximize its benefits by minimizing the opponent’s.</b>\n",
    "\n",
    "For example, in a game of tennis, <b>each agent wants to beat the other agent.</b>\n",
    "\n",
    "- <i>Mixed of both adversarial and cooperative</i>: like in our SoccerTwos environment, two agents are part of a team (blue or purple): they need to cooperate with each other and beat the opponent team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3967a7c",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif\" style=\"width:400px;\" title=\"Patchwork\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00d291",
   "metadata": {},
   "source": [
    "So now we might wonder: how can we design these multi-agent systems? Said differently, <b>how can we train agents in a multi-agent setting ?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a90b49",
   "metadata": {},
   "source": [
    "# What are the policy-based methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bf89c7",
   "metadata": {},
   "source": [
    "The main goal of Reinforcement learning is to <b>find the optimal policy $\\pi^{*}$   that will maximize the expected cumulative reward</b>. Because Reinforcement Learning is based on the <i>reward hypothesis</i>: <b>all goals can be described as the maximization of the expected cumulative reward.</b>\n",
    "\n",
    "For instance, in a soccer game (where you’re going to train the agents in two units), the goal is to win the game. We can describe this goal in reinforcement learning as <b>maximizing the number of goals scored</b> (when the ball crosses the goal line) into your opponent’s soccer goals. And <b>minimizing the number of goals in your soccer goals.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367adc6d",
   "metadata": {},
   "source": [
    "## Value-based, Policy-based, and Actor-critic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c451773",
   "metadata": {},
   "source": [
    "In the first unit, we saw two methods to find (or, most of the time, approximate) this optimal policy $\\pi^{*}$.\n",
    "\n",
    "- In <i>value-based methods</i>, we learn a value function.\n",
    "    - The idea is that an optimal value function leads to an optimal policy $\\pi^{*}$.\n",
    "     - Our objective is to <b>minimize the loss between the predicted and target value</b> to approximate the true  action-value function.\n",
    "    - We have a policy, but it’s implicit since it <b>is generated directly from the value function</b>. For instance, in Q-Learning, we used an (epsilon-)greedy policy.\n",
    "\n",
    "- On the other hand, in <i>policy-based methods</i>, we directly learn to approximate $\\pi^{*}$ without having to learn a value function.\n",
    "    - The idea is to <b>parameterize the policy</b>. For instance, using a neural network $\\pi_{\\theta}$ , this policy will output a probability distribution over actions (stochastic policy).\n",
    "    - <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/stochastic_policy.png\" style=\"width:400px;\" title=\"Stochastic Policy\">\n",
    "    - Our objective then is to <b>maximize the performance of the parameterized policy using gradient ascent.</b>\n",
    "    - To do that, we control the parameter $\\theta$ that will affect the distribution of actions over a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fd80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b368cbb",
   "metadata": {},
   "source": [
    "# The advantages and disadvantages of policy-gradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c58e23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd22c465",
   "metadata": {},
   "source": [
    "# Diving deeper into policy-gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a0fa2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76212e43",
   "metadata": {},
   "source": [
    "# (Optional) the Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f8235",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b16fcfc",
   "metadata": {},
   "source": [
    "# Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ca1ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48391d8b",
   "metadata": {},
   "source": [
    "# Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56955b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85544e5",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ed370",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75932773",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e718d7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc5b552",
   "metadata": {},
   "source": [
    "# Additional Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5261e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "790px",
    "left": "23px",
    "top": "111.125px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
